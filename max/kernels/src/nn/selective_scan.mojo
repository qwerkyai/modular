# ===----------------------------------------------------------------------=== #
# Copyright (c) 2025, Qwerky AI Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

# ===----------------------------------------------------------------------=== #
# Core selective scan kernel implementation
# ===----------------------------------------------------------------------=== #
# This file contains the core GPU and CPU kernel implementations for selective scan.
# 
# The kernel functions `selective_scan_fwd_gpu` and `selective_scan_fwd_cpu` are
# the actual kernels that perform the selective scan computation.
#
# For MAX framework integration, see MOGGKernelAPI.mojo.
# ===----------------------------------------------------------------------=== #

from gpu import block_dim, block_idx, thread_idx
from layout import Layout, LayoutTensor
from utils.index import IndexList
from memory import UnsafePointer
from algorithm import sync_parallelize
import math
from math import ceildiv, exp, exp2, rsqrt

# LOG2E constant for converting exp to exp2 (faster on GPU)
comptime LOG2E = 1.4426950408889634
comptime MAX_DSTATE = 16

# Optimized softplus - matches CUDA reference implementation
# Uses log1pf(expf(x)) for better numerical stability when x <= 20
fn softplus(val: Float32) -> Float32:
    if val > 20.0:
        return val
    # log1p(exp(x)) = log(1 + exp(x)) for better numerical stability
    var exp_val = math.exp(val)
    return math.log(1.0 + exp_val)

# Optimized sigmoid using fast approximation for large negative values
fn sigmoid(val: Float32) -> Float32:
    if val < -20.0:
        return 0.0
    var exp_neg = math.exp(-val)
    return 1.0 / (1.0 + exp_neg)

# Optimized SiLU (swish) activation
fn silu(val: Float32) -> Float32:
    if val < -20.0:
        return 0.0
    var exp_neg = math.exp(-val)
    return val / (1.0 + exp_neg)

# GPU kernel function - must be at module scope for compile_function_checked
fn selective_scan_fwd_gpu[
    kernel_dtype: DType,
    output_layout: Layout,
    x_layout: Layout,
    out_z_layout: Layout,
    u_layout: Layout,
    delta_layout: Layout,
    A_layout: Layout,
    B_layout: Layout,
    C_layout: Layout,
    D_layout: Layout,
    z_layout: Layout,
    delta_bias_layout: Layout,
](
    total_batch_dim: Int,
    batch: Int,
    dim: Int,
    seqlen: Int,
    dstate: Int,
    group_size: Int,
    delta_softplus: Int8,
    output: LayoutTensor[kernel_dtype, output_layout, MutAnyOrigin],
    x: LayoutTensor[kernel_dtype, x_layout, MutAnyOrigin],
    out_z: LayoutTensor[kernel_dtype, out_z_layout, MutAnyOrigin],
    u: LayoutTensor[kernel_dtype, u_layout, MutAnyOrigin],
    delta: LayoutTensor[kernel_dtype, delta_layout, MutAnyOrigin],
    A: LayoutTensor[kernel_dtype, A_layout, MutAnyOrigin],
    B: LayoutTensor[kernel_dtype, B_layout, MutAnyOrigin],
    C: LayoutTensor[kernel_dtype, C_layout, MutAnyOrigin],
    D: LayoutTensor[kernel_dtype, D_layout, MutAnyOrigin],
    z: LayoutTensor[kernel_dtype, z_layout, MutAnyOrigin],
    delta_bias: LayoutTensor[kernel_dtype, delta_bias_layout, MutAnyOrigin],
    # Strides
    output_b_stride: UInt32,
    output_d_stride: UInt32,
    output_t_stride: UInt32,
    x_b_stride: UInt32,
    x_d_stride: UInt32,
    x_chunk_stride: UInt32,
    x_n_stride: UInt32,
    out_z_b_stride: UInt32,
    out_z_d_stride: UInt32,
    out_z_t_stride: UInt32,
    u_b_stride: UInt32,
    u_d_stride: UInt32,
    u_t_stride: UInt32,
    delta_b_stride: UInt32,
    delta_d_stride: UInt32,
    delta_t_stride: UInt32,
    A_d_stride: UInt32,
    A_n_stride: UInt32,
    B_b_stride: UInt32,
    B_g_stride: UInt32,
    B_n_stride: UInt32,
    B_t_stride: UInt32,
    C_b_stride: UInt32,
    C_g_stride: UInt32,
    C_n_stride: UInt32,
    C_t_stride: UInt32,
    D_stride: UInt32,
    z_b_stride: UInt32,
    z_d_stride: UInt32,
    z_t_stride: UInt32,
    delta_bias_stride: UInt32,
):
    """GPU kernel for selective scan operation.
    
    Each thread processes one (batch, dim) pair and iterates through the sequence.
    """
    # Calculate which (batch, dim) this thread is responsible for
    var thread_id = block_dim.x * block_idx.x + thread_idx.x
    var thread_id_int = Int(thread_id)
    if thread_id_int >= total_batch_dim:
        return
        
    var b = thread_id_int // dim
    var d = thread_id_int % dim
    
    # Additional bounds checking
    if b >= batch or d >= dim:
        return
        
    var group_id = d // group_size
    
    # Local state storage (max dstate 16 to fit in registers)
    # Note: Using large SIMD sizes (e.g. 256) causes register spilling and massive performance loss
    var state = SIMD[DType.float32, MAX_DSTATE](0.0)
    var cum_a = SIMD[DType.float32, MAX_DSTATE](1.0)
    var cum_b = SIMD[DType.float32, MAX_DSTATE](0.0)
    
    # Pre-load A values for this dim and pre-multiply by LOG2E for faster exp2
    # This optimization converts exp(A * delta) to exp2(A * LOG2E * delta)
    # which is faster on GPUs
    var A_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
    var has_delta_bias = delta_bias.dim(0) > 0
    var delta_bias_val = Float32(0.0)
    if has_delta_bias:
        var bias_offset = UInt32(d) * delta_bias_stride
        delta_bias_val = Scalar[kernel_dtype](delta_bias.ptr.offset(bias_offset).load()).cast[DType.float32]()
    
    var has_D = D.dim(0) > 0
    var D_val = Float32(0.0)
    if has_D:
        var D_offset = UInt32(d) * D_stride
        D_val = Scalar[kernel_dtype](D.ptr.offset(D_offset).load()).cast[DType.float32]()
    
    var delta_softplus_bool = Bool(Int(delta_softplus) != 0)
    var has_z = z.dim(0) > 0
    var has_out_z = out_z.dim(0) > 0
    
    # Pre-multiply A by LOG2E for exp2 optimization
    for n in range(dstate):
        var A_offset = UInt32(d) * A_d_stride + UInt32(n) * A_n_stride
        A_vals[n] = Scalar[kernel_dtype](A.ptr.offset(A_offset).load()).cast[DType.float32]() * LOG2E

    var chunk_size = 2048
    var t_in_chunk = 0
    var chunk_idx = 0
    
    # Initialize running offsets for strength reduction
    var curr_u_offset = UInt32(b) * u_b_stride + UInt32(d) * u_d_stride
    var curr_delta_offset = UInt32(b) * delta_b_stride + UInt32(d) * delta_d_stride
    var curr_output_offset = UInt32(b) * output_b_stride + UInt32(d) * output_d_stride
    var curr_B_offset = UInt32(b) * B_b_stride + UInt32(group_id) * B_g_stride
    var curr_C_offset = UInt32(b) * C_b_stride + UInt32(group_id) * C_g_stride
    var curr_z_offset = UInt32(b) * z_b_stride + UInt32(d) * z_d_stride
    var curr_out_z_offset = UInt32(b) * out_z_b_stride + UInt32(d) * out_z_d_stride
    
    # Process sequence sequentially for this (batch, dim)
    # OPTIMIZED: Tiled loading with pre-loaded B/C tiles and buffered outputs
    comptime TILE_SIZE = 8  # Sweet spot: larger causes register spilling
    var aligned_seqlen = seqlen - (seqlen % TILE_SIZE)
    var t = 0
    
    # Check if we can use contiguous loads (stride == 1)
    var u_contiguous = u_t_stride == 1
    var delta_contiguous = delta_t_stride == 1
    var z_contiguous = z_t_stride == 1
    var B_contiguous = B_t_stride == 1
    var C_contiguous = C_t_stride == 1
    var output_contiguous = output_t_stride == 1
    
    # Fast path: Tiled loading for the aligned portion
    while t < aligned_seqlen:
        # Load u and delta vectors
        var u_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
        var delta_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
        var z_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
        
        if u_contiguous:
            u_vec = u.ptr.offset(curr_u_offset).load[width=TILE_SIZE]()
        else:
            for i in range(TILE_SIZE):
                u_vec[i] = u.ptr.offset(curr_u_offset + UInt32(i) * u_t_stride).load()
                
        if delta_contiguous:
            delta_vec = delta.ptr.offset(curr_delta_offset).load[width=TILE_SIZE]()
        else:
            for i in range(TILE_SIZE):
                delta_vec[i] = delta.ptr.offset(curr_delta_offset + UInt32(i) * delta_t_stride).load()
                
        if has_z:
            if z_contiguous:
                z_vec = z.ptr.offset(curr_z_offset).load[width=TILE_SIZE]()
            else:
                for i in range(TILE_SIZE):
                    z_vec[i] = z.ptr.offset(curr_z_offset + UInt32(i) * z_t_stride).load()
        
        # PRE-LOAD B/C TILES: Load B[n, t:t+TILE] and C[n, t:t+TILE] for all n
        # This avoids redundant address calculations inside the inner loop
        # Using flat arrays: B_tile[n * TILE_SIZE + i] = B[n, t+i]
        var B_tile_0 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_1 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_2 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_3 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_4 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_5 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_6 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_7 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_8 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_9 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_10 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_11 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_12 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_13 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_14 = SIMD[DType.float32, TILE_SIZE](0.0)
        var B_tile_15 = SIMD[DType.float32, TILE_SIZE](0.0)
        
        var C_tile_0 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_1 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_2 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_3 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_4 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_5 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_6 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_7 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_8 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_9 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_10 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_11 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_12 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_13 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_14 = SIMD[DType.float32, TILE_SIZE](0.0)
        var C_tile_15 = SIMD[DType.float32, TILE_SIZE](0.0)
        
        # Load B tiles - vector load if B is contiguous in t
        if B_contiguous:
            if dstate > 0:
                B_tile_0 = B.ptr.offset(curr_B_offset + UInt32(0) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 1:
                B_tile_1 = B.ptr.offset(curr_B_offset + UInt32(1) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 2:
                B_tile_2 = B.ptr.offset(curr_B_offset + UInt32(2) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 3:
                B_tile_3 = B.ptr.offset(curr_B_offset + UInt32(3) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 4:
                B_tile_4 = B.ptr.offset(curr_B_offset + UInt32(4) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 5:
                B_tile_5 = B.ptr.offset(curr_B_offset + UInt32(5) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 6:
                B_tile_6 = B.ptr.offset(curr_B_offset + UInt32(6) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 7:
                B_tile_7 = B.ptr.offset(curr_B_offset + UInt32(7) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 8:
                B_tile_8 = B.ptr.offset(curr_B_offset + UInt32(8) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 9:
                B_tile_9 = B.ptr.offset(curr_B_offset + UInt32(9) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 10:
                B_tile_10 = B.ptr.offset(curr_B_offset + UInt32(10) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 11:
                B_tile_11 = B.ptr.offset(curr_B_offset + UInt32(11) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 12:
                B_tile_12 = B.ptr.offset(curr_B_offset + UInt32(12) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 13:
                B_tile_13 = B.ptr.offset(curr_B_offset + UInt32(13) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 14:
                B_tile_14 = B.ptr.offset(curr_B_offset + UInt32(14) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 15:
                B_tile_15 = B.ptr.offset(curr_B_offset + UInt32(15) * B_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
        else:
            # Scalar fallback for non-contiguous B
            for i in range(TILE_SIZE):
                var b_base = curr_B_offset + UInt32(i) * B_t_stride
                if dstate > 0: B_tile_0[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(0) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 1: B_tile_1[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(1) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 2: B_tile_2[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(2) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 3: B_tile_3[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(3) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 4: B_tile_4[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(4) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 5: B_tile_5[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(5) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 6: B_tile_6[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(6) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 7: B_tile_7[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(7) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 8: B_tile_8[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(8) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 9: B_tile_9[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(9) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 10: B_tile_10[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(10) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 11: B_tile_11[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(11) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 12: B_tile_12[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(12) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 13: B_tile_13[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(13) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 14: B_tile_14[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(14) * B_n_stride).load()).cast[DType.float32]()
                if dstate > 15: B_tile_15[i] = Scalar[kernel_dtype](B.ptr.offset(b_base + UInt32(15) * B_n_stride).load()).cast[DType.float32]()
        
        # Load C tiles - vector load if C is contiguous in t
        if C_contiguous:
            if dstate > 0:
                C_tile_0 = C.ptr.offset(curr_C_offset + UInt32(0) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 1:
                C_tile_1 = C.ptr.offset(curr_C_offset + UInt32(1) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 2:
                C_tile_2 = C.ptr.offset(curr_C_offset + UInt32(2) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 3:
                C_tile_3 = C.ptr.offset(curr_C_offset + UInt32(3) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 4:
                C_tile_4 = C.ptr.offset(curr_C_offset + UInt32(4) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 5:
                C_tile_5 = C.ptr.offset(curr_C_offset + UInt32(5) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 6:
                C_tile_6 = C.ptr.offset(curr_C_offset + UInt32(6) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 7:
                C_tile_7 = C.ptr.offset(curr_C_offset + UInt32(7) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 8:
                C_tile_8 = C.ptr.offset(curr_C_offset + UInt32(8) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 9:
                C_tile_9 = C.ptr.offset(curr_C_offset + UInt32(9) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 10:
                C_tile_10 = C.ptr.offset(curr_C_offset + UInt32(10) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 11:
                C_tile_11 = C.ptr.offset(curr_C_offset + UInt32(11) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 12:
                C_tile_12 = C.ptr.offset(curr_C_offset + UInt32(12) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 13:
                C_tile_13 = C.ptr.offset(curr_C_offset + UInt32(13) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 14:
                C_tile_14 = C.ptr.offset(curr_C_offset + UInt32(14) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
            if dstate > 15:
                C_tile_15 = C.ptr.offset(curr_C_offset + UInt32(15) * C_n_stride).load[width=TILE_SIZE]().cast[DType.float32]()
        else:
            # Scalar fallback for non-contiguous C
            for i in range(TILE_SIZE):
                var c_base = curr_C_offset + UInt32(i) * C_t_stride
                if dstate > 0: C_tile_0[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(0) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 1: C_tile_1[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(1) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 2: C_tile_2[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(2) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 3: C_tile_3[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(3) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 4: C_tile_4[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(4) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 5: C_tile_5[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(5) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 6: C_tile_6[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(6) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 7: C_tile_7[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(7) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 8: C_tile_8[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(8) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 9: C_tile_9[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(9) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 10: C_tile_10[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(10) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 11: C_tile_11[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(11) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 12: C_tile_12[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(12) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 13: C_tile_13[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(13) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 14: C_tile_14[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(14) * C_n_stride).load()).cast[DType.float32]()
                if dstate > 15: C_tile_15[i] = Scalar[kernel_dtype](C.ptr.offset(c_base + UInt32(15) * C_n_stride).load()).cast[DType.float32]()
        
        # Buffer for output values to enable vector stores
        var output_buffer = SIMD[kernel_dtype, TILE_SIZE](0.0)
        var out_z_buffer = SIMD[kernel_dtype, TILE_SIZE](0.0)
        
        # Process tile with pre-loaded B/C values
        for i in range(TILE_SIZE):
            t_in_chunk += 1
            
            # Extract scalars from pre-loaded vectors
            var u_val = u_vec[i].cast[DType.float32]()
            var delta_val = delta_vec[i].cast[DType.float32]()
            
            # Apply delta bias and softplus
            if has_delta_bias:
                delta_val += delta_bias_val
            if delta_softplus_bool:
                delta_val = softplus(delta_val)
                
            var delta_u = delta_val * u_val
            
            # Extract B/C values for this timestep from pre-loaded tiles
            var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
            var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
            
            if dstate > 0: B_vals[0] = B_tile_0[i]
            if dstate > 1: B_vals[1] = B_tile_1[i]
            if dstate > 2: B_vals[2] = B_tile_2[i]
            if dstate > 3: B_vals[3] = B_tile_3[i]
            if dstate > 4: B_vals[4] = B_tile_4[i]
            if dstate > 5: B_vals[5] = B_tile_5[i]
            if dstate > 6: B_vals[6] = B_tile_6[i]
            if dstate > 7: B_vals[7] = B_tile_7[i]
            if dstate > 8: B_vals[8] = B_tile_8[i]
            if dstate > 9: B_vals[9] = B_tile_9[i]
            if dstate > 10: B_vals[10] = B_tile_10[i]
            if dstate > 11: B_vals[11] = B_tile_11[i]
            if dstate > 12: B_vals[12] = B_tile_12[i]
            if dstate > 13: B_vals[13] = B_tile_13[i]
            if dstate > 14: B_vals[14] = B_tile_14[i]
            if dstate > 15: B_vals[15] = B_tile_15[i]
            
            if dstate > 0: C_vals[0] = C_tile_0[i]
            if dstate > 1: C_vals[1] = C_tile_1[i]
            if dstate > 2: C_vals[2] = C_tile_2[i]
            if dstate > 3: C_vals[3] = C_tile_3[i]
            if dstate > 4: C_vals[4] = C_tile_4[i]
            if dstate > 5: C_vals[5] = C_tile_5[i]
            if dstate > 6: C_vals[6] = C_tile_6[i]
            if dstate > 7: C_vals[7] = C_tile_7[i]
            if dstate > 8: C_vals[8] = C_tile_8[i]
            if dstate > 9: C_vals[9] = C_tile_9[i]
            if dstate > 10: C_vals[10] = C_tile_10[i]
            if dstate > 11: C_vals[11] = C_tile_11[i]
            if dstate > 12: C_vals[12] = C_tile_12[i]
            if dstate > 13: C_vals[13] = C_tile_13[i]
            if dstate > 14: C_vals[14] = C_tile_14[i]
            if dstate > 15: C_vals[15] = C_tile_15[i]
            
            # SIMD Math
            var a_t = exp2(A_vals * delta_val)
            var b_t = B_vals * delta_u
            state = state * a_t + b_t
            var output_val = (state * C_vals).reduce_add()
            
            cum_b = cum_b * a_t + b_t
            cum_a = cum_a * a_t
            
            if has_D:
                output_val += D_val * u_val
            
            # Buffer output for vector store
            output_buffer[i] = output_val.cast[kernel_dtype]()
            
            if has_z:
                var z_val = z_vec[i].cast[DType.float32]()
                var out_z_val = output_val * silu(z_val)
                out_z_buffer[i] = out_z_val.cast[kernel_dtype]()
            
            # Checkpoint handling
            var current_t = t + i
            var is_chunk_boundary = (t_in_chunk == chunk_size)
            var is_last_step = (current_t == seqlen - 1)
            
            if is_chunk_boundary or is_last_step:
                for n in range(dstate):
                    var x_offset_a = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2) * Int(x_n_stride))
                    var x_offset_b = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2 + 1) * Int(x_n_stride))
                    x.ptr.offset(x_offset_a).store(Scalar[kernel_dtype](cum_a[n].cast[kernel_dtype]()))
                    x.ptr.offset(x_offset_b).store(Scalar[kernel_dtype](cum_b[n].cast[kernel_dtype]()))
                    cum_a[n] = 1.0
                    cum_b[n] = 0.0
                
                if is_chunk_boundary:
                    chunk_idx += 1
                    t_in_chunk = 0
        
        # Vector store outputs if contiguous
        if output_contiguous:
            output.ptr.offset(curr_output_offset).store[width=TILE_SIZE](output_buffer)
        else:
            for i in range(TILE_SIZE):
                var out_off = curr_output_offset + UInt32(i) * output_t_stride
                output.ptr.offset(out_off).store(output_buffer[i])
        
        if has_z and has_out_z:
            if out_z_t_stride == 1:
                out_z.ptr.offset(curr_out_z_offset).store[width=TILE_SIZE](out_z_buffer)
            else:
                for i in range(TILE_SIZE):
                    var out_z_off = curr_out_z_offset + UInt32(i) * out_z_t_stride
                    out_z.ptr.offset(out_z_off).store(out_z_buffer[i])

        # Advance global offsets by TILE_SIZE
        curr_u_offset += u_t_stride * UInt32(TILE_SIZE)
        curr_delta_offset += delta_t_stride * UInt32(TILE_SIZE)
        curr_output_offset += output_t_stride * UInt32(TILE_SIZE)
        curr_B_offset += B_t_stride * UInt32(TILE_SIZE)
        curr_C_offset += C_t_stride * UInt32(TILE_SIZE)
        curr_z_offset += z_t_stride * UInt32(TILE_SIZE)
        curr_out_z_offset += out_z_t_stride * UInt32(TILE_SIZE)
        
        t += TILE_SIZE

    # Tail loop (scalar)
    while t < seqlen:
        t_in_chunk += 1
        # ... (same body as original loop) ...
        # Copied from original for correctness
        var u_val = Scalar[kernel_dtype](u.ptr.offset(curr_u_offset).load()).cast[DType.float32]()
        var delta_val = Scalar[kernel_dtype](delta.ptr.offset(curr_delta_offset).load()).cast[DType.float32]()
        if has_delta_bias: delta_val += delta_bias_val
        if delta_softplus_bool: delta_val = softplus(delta_val)
        var delta_u = delta_val * u_val
        var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        for n in range(dstate):
            B_vals[n] = Scalar[kernel_dtype](B.ptr.offset(curr_B_offset + UInt32(n) * B_n_stride).load()).cast[DType.float32]()
            C_vals[n] = Scalar[kernel_dtype](C.ptr.offset(curr_C_offset + UInt32(n) * C_n_stride).load()).cast[DType.float32]()
        var a_t = exp2(A_vals * delta_val)
        var b_t = B_vals * delta_u
        state = state * a_t + b_t
        var output_val = (state * C_vals).reduce_add()
        cum_b = cum_b * a_t + b_t
        cum_a = cum_a * a_t
        if has_D: output_val += D_val * u_val
        output.ptr.offset(curr_output_offset).store(Scalar[kernel_dtype](output_val.cast[kernel_dtype]()))
        if has_z:
            var z_val = Scalar[kernel_dtype](z.ptr.offset(curr_z_offset).load()).cast[DType.float32]()
            var out_z_val = output_val * silu(z_val)
            if has_out_z: out_z.ptr.offset(curr_out_z_offset).store(Scalar[kernel_dtype](out_z_val.cast[kernel_dtype]()))
        
        curr_u_offset += u_t_stride
        curr_delta_offset += delta_t_stride
        curr_output_offset += output_t_stride
        curr_B_offset += B_t_stride
        curr_C_offset += C_t_stride
        curr_z_offset += z_t_stride
        curr_out_z_offset += out_z_t_stride
        
        var is_chunk_boundary = (t_in_chunk == chunk_size)
        var is_last_step = (t == seqlen - 1)
        if is_chunk_boundary or is_last_step:
            for n in range(dstate):
                var x_offset_a = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2) * Int(x_n_stride))
                var x_offset_b = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2 + 1) * Int(x_n_stride))
                x.ptr.offset(x_offset_a).store(Scalar[kernel_dtype](cum_a[n].cast[kernel_dtype]()))
                x.ptr.offset(x_offset_b).store(Scalar[kernel_dtype](cum_b[n].cast[kernel_dtype]()))
                cum_a[n] = 1.0
                cum_b[n] = 0.0
            if is_chunk_boundary:
                chunk_idx += 1
                t_in_chunk = 0
        t += 1


# ===----------------------------------------------------------------------=== #
# Selective Scan Update Kernel (Single Step / Autoregressive)
# ===----------------------------------------------------------------------=== #
# This kernel is used for incremental/autoregressive inference where you
# process one token at a time and update the hidden state.
#
# Reference: mamba_ssm/ops/triton/selective_state_update.py
#
# Algorithm:
#   dt = dt + dt_bias (if has_dt_bias)
#   dt = softplus(dt) (if dt_softplus)
#   dA = exp(A * dt)
#   dB = B * dt
#   state = state * dA + dB * x
#   out = sum(state * C, axis=-1)
#   out += x * D (if has_D)
#   out *= z * sigmoid(z) (if has_z)
# ===----------------------------------------------------------------------=== #

fn selective_scan_update_gpu[
    kernel_dtype: DType,
    state_out_layout: Layout,
    output_layout: Layout,
    state_in_layout: Layout,
    x_layout: Layout,
    dt_layout: Layout,
    A_layout: Layout,
    B_layout: Layout,
    C_layout: Layout,
    D_layout: Layout,
    z_layout: Layout,
    dt_bias_layout: Layout,
](
    total_batch_dim: Int,
    batch: Int,
    dim: Int,
    dstate: Int,
    group_size: Int,
    delta_softplus: Int8,
    # Outputs first (MAX convention)
    state_out: LayoutTensor[kernel_dtype, state_out_layout, MutAnyOrigin],
    output: LayoutTensor[kernel_dtype, output_layout, MutAnyOrigin],
    # Inputs
    state_in: LayoutTensor[kernel_dtype, state_in_layout, MutAnyOrigin],
    x: LayoutTensor[kernel_dtype, x_layout, MutAnyOrigin],
    dt: LayoutTensor[kernel_dtype, dt_layout, MutAnyOrigin],
    A: LayoutTensor[kernel_dtype, A_layout, MutAnyOrigin],
    B: LayoutTensor[kernel_dtype, B_layout, MutAnyOrigin],
    C: LayoutTensor[kernel_dtype, C_layout, MutAnyOrigin],
    D: LayoutTensor[kernel_dtype, D_layout, MutAnyOrigin],
    z: LayoutTensor[kernel_dtype, z_layout, MutAnyOrigin],
    dt_bias: LayoutTensor[kernel_dtype, dt_bias_layout, MutAnyOrigin],
    # Strides for state_out
    state_out_b_stride: UInt32,
    state_out_d_stride: UInt32,
    state_out_n_stride: UInt32,
    output_b_stride: UInt32,
    output_d_stride: UInt32,
    # Strides for state_in
    state_in_b_stride: UInt32,
    state_in_d_stride: UInt32,
    state_in_n_stride: UInt32,
    x_b_stride: UInt32,
    x_d_stride: UInt32,
    dt_b_stride: UInt32,
    dt_d_stride: UInt32,
    A_d_stride: UInt32,
    A_n_stride: UInt32,
    B_b_stride: UInt32,
    B_g_stride: UInt32,
    B_n_stride: UInt32,
    C_b_stride: UInt32,
    C_g_stride: UInt32,
    C_n_stride: UInt32,
    D_stride: UInt32,
    z_b_stride: UInt32,
    z_d_stride: UInt32,
    dt_bias_stride: UInt32,
):
    """GPU kernel for selective scan update (single step).
    
    Each thread processes one (batch, dim) pair.
    Reads initial state from state_in, writes updated state to state_out.
    
    Args:
        state_out: Output SSM state tensor (batch, dim, dstate)
        output: Output tensor (batch, dim)
        state_in: Input SSM state tensor (batch, dim, dstate)
        x: Input tensor (batch, dim)
        dt: Delta/timestep tensor (batch, dim)
        A: State transition matrix (dim, dstate)
        B: Input projection (batch, dstate)
        C: Output projection (batch, dstate)
        D: Optional skip connection (dim,)
        z: Optional gate (batch, dim)
        dt_bias: Optional delta bias (dim,)
    """
    # Calculate which (batch, dim) this thread is responsible for
    var thread_id = block_dim.x * block_idx.x + thread_idx.x
    var thread_id_int = Int(thread_id)
    if thread_id_int >= total_batch_dim:
        return
        
    var b = thread_id_int // dim
    var d = thread_id_int % dim
    
    # Additional bounds checking
    if b >= batch or d >= dim:
        return
    
    # Compute group_id for this dimension
    var group_id = d // group_size
    
    # Load dt value
    var dt_offset = UInt32(b) * dt_b_stride + UInt32(d) * dt_d_stride
    var dt_val = Scalar[kernel_dtype](dt.ptr.offset(dt_offset).load()).cast[DType.float32]()
    
    # Apply dt_bias if present
    var has_dt_bias = dt_bias.dim(0) > 0
    if has_dt_bias:
        var bias_offset = UInt32(d) * dt_bias_stride
        var bias_val = Scalar[kernel_dtype](dt_bias.ptr.offset(bias_offset).load()).cast[DType.float32]()
        dt_val += bias_val
    
    # Apply softplus if requested
    var delta_softplus_bool = Bool(Int(delta_softplus) != 0)
    if delta_softplus_bool:
        dt_val = softplus(dt_val)
    
    # Load x value
    var x_offset = UInt32(b) * x_b_stride + UInt32(d) * x_d_stride
    var x_val = Scalar[kernel_dtype](x.ptr.offset(x_offset).load()).cast[DType.float32]()
    
    # Load A values for this dim and pre-multiply by LOG2E for faster exp2
    var A_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
    for n in range(dstate):
        var A_offset = UInt32(d) * A_d_stride + UInt32(n) * A_n_stride
        A_vals[n] = Scalar[kernel_dtype](A.ptr.offset(A_offset).load()).cast[DType.float32]() * LOG2E
    
    # Compute dA = exp2(A * LOG2E * dt) = exp(A * dt)
    var dA = exp2(A_vals * dt_val)
    
    # Load B values using group_id
    var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
    for n in range(dstate):
        var B_offset = UInt32(b) * B_b_stride + UInt32(group_id) * B_g_stride + UInt32(n) * B_n_stride
        B_vals[n] = Scalar[kernel_dtype](B.ptr.offset(B_offset).load()).cast[DType.float32]()
    
    # Compute dB = B * dt
    var dB = B_vals * dt_val
    
    # Load current state from state_in
    var state_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
    for n in range(dstate):
        var state_offset = UInt32(b) * state_in_b_stride + UInt32(d) * state_in_d_stride + UInt32(n) * state_in_n_stride
        state_vals[n] = Scalar[kernel_dtype](state_in.ptr.offset(state_offset).load()).cast[DType.float32]()
    
    # Update state: state = state * dA + dB * x
    state_vals = state_vals * dA + dB * x_val
    
    # Store updated state to state_out
    for n in range(dstate):
        var state_offset = UInt32(b) * state_out_b_stride + UInt32(d) * state_out_d_stride + UInt32(n) * state_out_n_stride
        state_out.ptr.offset(state_offset).store(Scalar[kernel_dtype](state_vals[n].cast[kernel_dtype]()))
    
    # Load C values using group_id
    var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
    for n in range(dstate):
        var C_offset = UInt32(b) * C_b_stride + UInt32(group_id) * C_g_stride + UInt32(n) * C_n_stride
        C_vals[n] = Scalar[kernel_dtype](C.ptr.offset(C_offset).load()).cast[DType.float32]()
    
    # Compute output: out = sum(state * C, axis=-1)
    var out_val = (state_vals * C_vals).reduce_add()
    
    # Add skip connection if D is present
    var has_D = D.dim(0) > 0
    if has_D:
        var D_offset = UInt32(d) * D_stride
        var D_val = Scalar[kernel_dtype](D.ptr.offset(D_offset).load()).cast[DType.float32]()
        out_val += x_val * D_val
    
    # Apply gating if z is present
    var has_z = z.dim(0) > 0
    if has_z:
        var z_offset = UInt32(b) * z_b_stride + UInt32(d) * z_d_stride
        var z_val = Scalar[kernel_dtype](z.ptr.offset(z_offset).load()).cast[DType.float32]()
        out_val *= z_val * sigmoid(z_val)  # z * sigmoid(z) = silu(z) but formulated differently
    
    # Store output
    var out_offset = UInt32(b) * output_b_stride + UInt32(d) * output_d_stride
    output.ptr.offset(out_offset).store(Scalar[kernel_dtype](out_val.cast[kernel_dtype]()))


fn selective_scan_update_cpu[
    kernel_dtype: DType,
    state_out_layout: Layout,
    output_layout: Layout,
    state_in_layout: Layout,
    x_layout: Layout,
    dt_layout: Layout,
    A_layout: Layout,
    B_layout: Layout,
    C_layout: Layout,
    D_layout: Layout,
    z_layout: Layout,
    dt_bias_layout: Layout,
](
    batch: Int,
    dim: Int,
    dstate: Int,
    group_size: Int,
    delta_softplus: Int8,
    # Outputs first (MAX convention)
    state_out: LayoutTensor[kernel_dtype, state_out_layout, MutAnyOrigin],
    output: LayoutTensor[kernel_dtype, output_layout, MutAnyOrigin],
    # Inputs
    state_in: LayoutTensor[kernel_dtype, state_in_layout, MutAnyOrigin],
    x: LayoutTensor[kernel_dtype, x_layout, MutAnyOrigin],
    dt: LayoutTensor[kernel_dtype, dt_layout, MutAnyOrigin],
    A: LayoutTensor[kernel_dtype, A_layout, MutAnyOrigin],
    B: LayoutTensor[kernel_dtype, B_layout, MutAnyOrigin],
    C: LayoutTensor[kernel_dtype, C_layout, MutAnyOrigin],
    D: LayoutTensor[kernel_dtype, D_layout, MutAnyOrigin],
    z: LayoutTensor[kernel_dtype, z_layout, MutAnyOrigin],
    dt_bias: LayoutTensor[kernel_dtype, dt_bias_layout, MutAnyOrigin],
    # Strides for state_out
    state_out_b_stride: UInt32,
    state_out_d_stride: UInt32,
    state_out_n_stride: UInt32,
    output_b_stride: UInt32,
    output_d_stride: UInt32,
    # Strides for state_in
    state_in_b_stride: UInt32,
    state_in_d_stride: UInt32,
    state_in_n_stride: UInt32,
    x_b_stride: UInt32,
    x_d_stride: UInt32,
    dt_b_stride: UInt32,
    dt_d_stride: UInt32,
    A_d_stride: UInt32,
    A_n_stride: UInt32,
    B_b_stride: UInt32,
    B_g_stride: UInt32,
    B_n_stride: UInt32,
    C_b_stride: UInt32,
    C_g_stride: UInt32,
    C_n_stride: UInt32,
    D_stride: UInt32,
    z_b_stride: UInt32,
    z_d_stride: UInt32,
    dt_bias_stride: UInt32,
):
    """CPU kernel for selective scan update (single step).
    
    Parallelized over batch and dimension.
    """
    var has_dt_bias = dt_bias.dim(0) > 0
    var has_D = D.dim(0) > 0
    var has_z = z.dim(0) > 0
    var delta_softplus_bool = Bool(Int(delta_softplus) != 0)
    
    @parameter
    fn worker(idx: Int):
        var b = idx // dim
        var d = idx % dim
        
        # Compute group_id for this dimension
        var group_id = d // group_size
        
        # Load dt value
        var dt_offset = UInt32(b) * dt_b_stride + UInt32(d) * dt_d_stride
        var dt_val = Scalar[kernel_dtype](dt.ptr.offset(dt_offset).load()).cast[DType.float32]()
        
        # Apply dt_bias if present
        if has_dt_bias:
            var bias_offset = UInt32(d) * dt_bias_stride
            var bias_val = Scalar[kernel_dtype](dt_bias.ptr.offset(bias_offset).load()).cast[DType.float32]()
            dt_val += bias_val
        
        # Apply softplus if requested
        if delta_softplus_bool:
            dt_val = softplus(dt_val)
        
        # Load x value
        var x_offset = UInt32(b) * x_b_stride + UInt32(d) * x_d_stride
        var x_val = Scalar[kernel_dtype](x.ptr.offset(x_offset).load()).cast[DType.float32]()
        
        # Load A values and pre-multiply by LOG2E
        var A_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        for n in range(dstate):
            var A_offset = UInt32(d) * A_d_stride + UInt32(n) * A_n_stride
            A_vals[n] = Scalar[kernel_dtype](A.ptr.offset(A_offset).load()).cast[DType.float32]() * LOG2E
        
        # Compute dA
        var dA = exp2(A_vals * dt_val)
        
        # Load B values using group_id
        var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        for n in range(dstate):
            var B_offset = UInt32(b) * B_b_stride + UInt32(group_id) * B_g_stride + UInt32(n) * B_n_stride
            B_vals[n] = Scalar[kernel_dtype](B.ptr.offset(B_offset).load()).cast[DType.float32]()
        
        # Compute dB
        var dB = B_vals * dt_val
        
        # Load current state from state_in
        var state_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        for n in range(dstate):
            var state_offset = UInt32(b) * state_in_b_stride + UInt32(d) * state_in_d_stride + UInt32(n) * state_in_n_stride
            state_vals[n] = Scalar[kernel_dtype](state_in.ptr.offset(state_offset).load()).cast[DType.float32]()
        
        # Update state
        state_vals = state_vals * dA + dB * x_val
        
        # Store updated state to state_out
        for n in range(dstate):
            var state_offset = UInt32(b) * state_out_b_stride + UInt32(d) * state_out_d_stride + UInt32(n) * state_out_n_stride
            state_out.ptr.offset(state_offset).store(Scalar[kernel_dtype](state_vals[n].cast[kernel_dtype]()))
        
        # Load C values using group_id
        var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        for n in range(dstate):
            var C_offset = UInt32(b) * C_b_stride + UInt32(group_id) * C_g_stride + UInt32(n) * C_n_stride
            C_vals[n] = Scalar[kernel_dtype](C.ptr.offset(C_offset).load()).cast[DType.float32]()
        
        # Compute output
        var out_val = (state_vals * C_vals).reduce_add()
        
        # Add skip connection
        if has_D:
            var D_offset = UInt32(d) * D_stride
            var D_val = Scalar[kernel_dtype](D.ptr.offset(D_offset).load()).cast[DType.float32]()
            out_val += x_val * D_val
        
        # Apply gating
        if has_z:
            var z_offset = UInt32(b) * z_b_stride + UInt32(d) * z_d_stride
            var z_val = Scalar[kernel_dtype](z.ptr.offset(z_offset).load()).cast[DType.float32]()
            out_val *= z_val * sigmoid(z_val)
        
        # Store output
        var out_offset = UInt32(b) * output_b_stride + UInt32(d) * output_d_stride
        output.ptr.offset(out_offset).store(Scalar[kernel_dtype](out_val.cast[kernel_dtype]()))

    sync_parallelize[worker](batch * dim)


fn selective_scan_fwd_cpu[
    kernel_dtype: DType,
    output_layout: Layout,
    x_layout: Layout,
    out_z_layout: Layout,
    u_layout: Layout,
    delta_layout: Layout,
    A_layout: Layout,
    B_layout: Layout,
    C_layout: Layout,
    D_layout: Layout,
    z_layout: Layout,
    delta_bias_layout: Layout,
](
    batch: Int,
    dim: Int,
    seqlen: Int,
    dstate: Int,
    group_size: Int,
    delta_softplus: Int8,
    output: LayoutTensor[kernel_dtype, output_layout, MutAnyOrigin],
    x: LayoutTensor[kernel_dtype, x_layout, MutAnyOrigin],
    out_z: LayoutTensor[kernel_dtype, out_z_layout, MutAnyOrigin],
    u: LayoutTensor[kernel_dtype, u_layout, MutAnyOrigin],
    delta: LayoutTensor[kernel_dtype, delta_layout, MutAnyOrigin],
    A: LayoutTensor[kernel_dtype, A_layout, MutAnyOrigin],
    B: LayoutTensor[kernel_dtype, B_layout, MutAnyOrigin],
    C: LayoutTensor[kernel_dtype, C_layout, MutAnyOrigin],
    D: LayoutTensor[kernel_dtype, D_layout, MutAnyOrigin],
    z: LayoutTensor[kernel_dtype, z_layout, MutAnyOrigin],
    delta_bias: LayoutTensor[kernel_dtype, delta_bias_layout, MutAnyOrigin],
    # Strides
    output_b_stride: UInt32,
    output_d_stride: UInt32,
    output_t_stride: UInt32,
    x_b_stride: UInt32,
    x_d_stride: UInt32,
    x_chunk_stride: UInt32,
    x_n_stride: UInt32,
    out_z_b_stride: UInt32,
    out_z_d_stride: UInt32,
    out_z_t_stride: UInt32,
    u_b_stride: UInt32,
    u_d_stride: UInt32,
    u_t_stride: UInt32,
    delta_b_stride: UInt32,
    delta_d_stride: UInt32,
    delta_t_stride: UInt32,
    A_d_stride: UInt32,
    A_n_stride: UInt32,
    B_b_stride: UInt32,
    B_g_stride: UInt32,
    B_n_stride: UInt32,
    B_t_stride: UInt32,
    C_b_stride: UInt32,
    C_g_stride: UInt32,
    C_n_stride: UInt32,
    C_t_stride: UInt32,
    D_stride: UInt32,
    z_b_stride: UInt32,
    z_d_stride: UInt32,
    z_t_stride: UInt32,
    delta_bias_stride: UInt32,
):
    """CPU kernel for selective scan operation.
    
    Parallelized over batch and dimension.
    """
    
    @parameter
    fn worker(idx: Int):
        var b = idx // dim
        var d = idx % dim
        
        # Bounds checking
        if b >= batch or d >= dim:
            return
        
        var group_id = d // group_size
        
        # Local state storage (max dstate 16)
        var state = SIMD[DType.float32, MAX_DSTATE](0.0)
        var cum_a = SIMD[DType.float32, MAX_DSTATE](1.0)
        var cum_b = SIMD[DType.float32, MAX_DSTATE](0.0)
        
        # Pre-load A values for this dim and pre-multiply by LOG2E for faster exp2
        var A_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        var has_delta_bias = delta_bias.dim(0) > 0
        var delta_bias_val = Float32(0.0)
        if has_delta_bias:
            var bias_offset = UInt32(d * Int(delta_bias_stride))
            delta_bias_val = Scalar[kernel_dtype](delta_bias.ptr.offset(bias_offset).load()).cast[DType.float32]()
        
        var has_D = D.dim(0) > 0
        var D_val = Float32(0.0)
        if has_D:
            var D_offset = UInt32(d * Int(D_stride))
            D_val = Scalar[kernel_dtype](D.ptr.offset(D_offset).load()).cast[DType.float32]()
        
        var delta_softplus_bool = Bool(Int(delta_softplus) != 0)
        var has_z = z.dim(0) > 0
        var has_out_z = out_z.dim(0) > 0
        
        for n in range(dstate):
            var A_offset = UInt32(d * Int(A_d_stride)) + UInt32(n * Int(A_n_stride))
            A_vals[n] = Scalar[kernel_dtype](A.ptr.offset(A_offset).load()).cast[DType.float32]() * LOG2E

        var chunk_size = 2048
        var t_in_chunk = 0
        var chunk_idx = 0
        
        var curr_u_offset = UInt32(b * Int(u_b_stride) + d * Int(u_d_stride))
        var curr_delta_offset = UInt32(b * Int(delta_b_stride) + d * Int(delta_d_stride))
        var curr_output_offset = UInt32(b * Int(output_b_stride) + d * Int(output_d_stride))
        var curr_B_offset = UInt32(b * Int(B_b_stride) + group_id * Int(B_g_stride))
        var curr_C_offset = UInt32(b * Int(C_b_stride) + group_id * Int(C_g_stride))
        var curr_z_offset = UInt32(b * Int(z_b_stride) + d * Int(z_d_stride))
        var curr_out_z_offset = UInt32(b * Int(out_z_b_stride) + d * Int(out_z_d_stride))
        
        comptime TILE_SIZE = 4
        var aligned_seqlen = seqlen - (seqlen % TILE_SIZE)
        var t = 0
        
        while t < aligned_seqlen:
            var u_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
            var delta_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
            var z_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
            
            if u_t_stride == 1:
                u_vec = u.ptr.offset(curr_u_offset).load[width=TILE_SIZE]()
            else:
                for i in range(TILE_SIZE):
                    u_vec[i] = u.ptr.offset(curr_u_offset + UInt32(i * Int(u_t_stride))).load()
                    
            if delta_t_stride == 1:
                delta_vec = delta.ptr.offset(curr_delta_offset).load[width=TILE_SIZE]()
            else:
                for i in range(TILE_SIZE):
                    delta_vec[i] = delta.ptr.offset(curr_delta_offset + UInt32(i * Int(delta_t_stride))).load()
                    
            if has_z:
                if z_t_stride == 1:
                    z_vec = z.ptr.offset(curr_z_offset).load[width=TILE_SIZE]()
                else:
                    for i in range(TILE_SIZE):
                        z_vec[i] = z.ptr.offset(curr_z_offset + UInt32(i * Int(z_t_stride))).load()
            
            for i in range(TILE_SIZE):
                t_in_chunk += 1
                
                var u_val = u_vec[i].cast[DType.float32]()
                var delta_val = delta_vec[i].cast[DType.float32]()
                
                if has_delta_bias:
                    delta_val += delta_bias_val
                
                if delta_softplus_bool:
                    delta_val = softplus(delta_val)
                    
                var delta_u = delta_val * u_val
                
                var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
                var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
                
                for n in range(dstate):
                    var b_off = curr_B_offset + UInt32(i * Int(B_t_stride)) + UInt32(n * Int(B_n_stride))
                    var c_off = curr_C_offset + UInt32(i * Int(C_t_stride)) + UInt32(n * Int(C_n_stride))
                    
                    B_vals[n] = Scalar[kernel_dtype](B.ptr.offset(b_off).load()).cast[DType.float32]()
                    C_vals[n] = Scalar[kernel_dtype](C.ptr.offset(c_off).load()).cast[DType.float32]()
                
                var a_t = exp2(A_vals * delta_val)
                var b_t = B_vals * delta_u
                state = state * a_t + b_t
                var output_val = (state * C_vals).reduce_add()
                
                cum_b = cum_b * a_t + b_t
                cum_a = cum_a * a_t
                
                if has_D:
                    output_val += D_val * u_val
                
                var final_val = output_val.cast[kernel_dtype]()
                
                if has_z:
                    var z_val = z_vec[i].cast[DType.float32]()
                    var out_z_val = output_val * silu(z_val)
                    if has_out_z:
                        var out_z_off = curr_out_z_offset + UInt32(i * Int(out_z_t_stride))
                        out_z.ptr.offset(out_z_off).store(Scalar[kernel_dtype](out_z_val.cast[kernel_dtype]()))
                
                var out_off = curr_output_offset + UInt32(i * Int(output_t_stride))
                output.ptr.offset(out_off).store(Scalar[kernel_dtype](final_val))
                
                var is_chunk_boundary = (t_in_chunk == chunk_size)
                var current_t = t + i
                var is_last_step = (current_t == seqlen - 1)
                
                if is_chunk_boundary or is_last_step:
                    for n in range(dstate):
                        var x_offset_a = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2) * Int(x_n_stride))
                        var x_offset_b = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2 + 1) * Int(x_n_stride))
                        x.ptr.offset(x_offset_a).store(Scalar[kernel_dtype](cum_a[n].cast[kernel_dtype]()))
                        x.ptr.offset(x_offset_b).store(Scalar[kernel_dtype](cum_b[n].cast[kernel_dtype]()))
                        cum_a[n] = 1.0
                        cum_b[n] = 0.0
                    
                    if is_chunk_boundary:
                        chunk_idx += 1
                        t_in_chunk = 0

            curr_u_offset += u_t_stride * UInt32(TILE_SIZE)
            curr_delta_offset += delta_t_stride * UInt32(TILE_SIZE)
            curr_output_offset += output_t_stride * UInt32(TILE_SIZE)
            curr_B_offset += B_t_stride * UInt32(TILE_SIZE)
            curr_C_offset += C_t_stride * UInt32(TILE_SIZE)
            curr_z_offset += z_t_stride * UInt32(TILE_SIZE)
            curr_out_z_offset += out_z_t_stride * UInt32(TILE_SIZE)
            
            t += TILE_SIZE

        while t < seqlen:
            t_in_chunk += 1
            var u_val = Scalar[kernel_dtype](u.ptr.offset(curr_u_offset).load()).cast[DType.float32]()
            var delta_val = Scalar[kernel_dtype](delta.ptr.offset(curr_delta_offset).load()).cast[DType.float32]()
            if has_delta_bias: delta_val += delta_bias_val
            if delta_softplus_bool: delta_val = softplus(delta_val)
            var delta_u = delta_val * u_val
            var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
            var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
            for n in range(dstate):
                B_vals[n] = Scalar[kernel_dtype](B.ptr.offset(curr_B_offset + UInt32(n) * B_n_stride).load()).cast[DType.float32]()
                C_vals[n] = Scalar[kernel_dtype](C.ptr.offset(curr_C_offset + UInt32(n) * C_n_stride).load()).cast[DType.float32]()
            var a_t = exp2(A_vals * delta_val)
            var b_t = B_vals * delta_u
            state = state * a_t + b_t
            var output_val = (state * C_vals).reduce_add()
            cum_b = cum_b * a_t + b_t
            cum_a = cum_a * a_t
            if has_D: output_val += D_val * u_val
            output.ptr.offset(curr_output_offset).store(Scalar[kernel_dtype](output_val.cast[kernel_dtype]()))
            if has_z:
                var z_val = Scalar[kernel_dtype](z.ptr.offset(curr_z_offset).load()).cast[DType.float32]()
                var out_z_val = output_val * silu(z_val)
                if has_out_z: out_z.ptr.offset(curr_out_z_offset).store(Scalar[kernel_dtype](out_z_val.cast[kernel_dtype]()))
            
            curr_u_offset += u_t_stride
            curr_delta_offset += delta_t_stride
            curr_output_offset += output_t_stride
            curr_B_offset += B_t_stride
            curr_C_offset += C_t_stride
            curr_z_offset += z_t_stride
            curr_out_z_offset += out_z_t_stride
            
            var is_chunk_boundary = (t_in_chunk == chunk_size)
            var is_last_step = (t == seqlen - 1)
            if is_chunk_boundary or is_last_step:
                for n in range(dstate):
                    var x_offset_a = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2) * Int(x_n_stride))
                    var x_offset_b = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2 + 1) * Int(x_n_stride))
                    x.ptr.offset(x_offset_a).store(Scalar[kernel_dtype](cum_a[n].cast[kernel_dtype]()))
                    x.ptr.offset(x_offset_b).store(Scalar[kernel_dtype](cum_b[n].cast[kernel_dtype]()))
                    cum_a[n] = 1.0
                    cum_b[n] = 0.0
                if is_chunk_boundary:
                    chunk_idx += 1
                    t_in_chunk = 0
            t += 1

    sync_parallelize[worker](batch * dim)


# ===----------------------------------------------------------------------=== #
# SSD Combined: Selective Scan Discrete Combined Operation
# ===----------------------------------------------------------------------=== #
# This kernel combines selective scan with normalization and residual connections.
# It performs: norm(residual + selective_scan(input))
# This is a fused operation for better performance in Mamba blocks.
# ===----------------------------------------------------------------------=== #

fn ssd_combined_gpu[
    kernel_dtype: DType,
    output_layout: Layout,
    x_layout: Layout,
    out_z_layout: Layout,
    residual_layout: Layout,
    u_layout: Layout,
    delta_layout: Layout,
    A_layout: Layout,
    B_layout: Layout,
    C_layout: Layout,
    D_layout: Layout,
    z_layout: Layout,
    delta_bias_layout: Layout,
    gamma_layout: Layout,
](
    total_batch_dim: Int,
    batch: Int,
    dim: Int,
    seqlen: Int,
    dstate: Int,
    group_size: Int,
    delta_softplus: Int8,
    output: LayoutTensor[kernel_dtype, output_layout, MutAnyOrigin],
    x: LayoutTensor[kernel_dtype, x_layout, MutAnyOrigin],
    out_z: LayoutTensor[kernel_dtype, out_z_layout, MutAnyOrigin],
    residual: LayoutTensor[kernel_dtype, residual_layout, MutAnyOrigin],
    u: LayoutTensor[kernel_dtype, u_layout, MutAnyOrigin],
    delta: LayoutTensor[kernel_dtype, delta_layout, MutAnyOrigin],
    A: LayoutTensor[kernel_dtype, A_layout, MutAnyOrigin],
    B: LayoutTensor[kernel_dtype, B_layout, MutAnyOrigin],
    C: LayoutTensor[kernel_dtype, C_layout, MutAnyOrigin],
    D: LayoutTensor[kernel_dtype, D_layout, MutAnyOrigin],
    z: LayoutTensor[kernel_dtype, z_layout, MutAnyOrigin],
    delta_bias: LayoutTensor[kernel_dtype, delta_bias_layout, MutAnyOrigin],
    gamma: LayoutTensor[kernel_dtype, gamma_layout, MutAnyOrigin],
    epsilon: Scalar[kernel_dtype],
    weight_offset: Scalar[kernel_dtype],
    # Strides
    output_b_stride: UInt32,
    output_d_stride: UInt32,
    output_t_stride: UInt32,
    x_b_stride: UInt32,
    x_d_stride: UInt32,
    x_chunk_stride: UInt32,
    x_n_stride: UInt32,
    out_z_b_stride: UInt32,
    out_z_d_stride: UInt32,
    out_z_t_stride: UInt32,
    residual_b_stride: UInt32,
    residual_d_stride: UInt32,
    residual_t_stride: UInt32,
    u_b_stride: UInt32,
    u_d_stride: UInt32,
    u_t_stride: UInt32,
    delta_b_stride: UInt32,
    delta_d_stride: UInt32,
    delta_t_stride: UInt32,
    A_d_stride: UInt32,
    A_n_stride: UInt32,
    B_b_stride: UInt32,
    B_g_stride: UInt32,
    B_n_stride: UInt32,
    B_t_stride: UInt32,
    C_b_stride: UInt32,
    C_g_stride: UInt32,
    C_n_stride: UInt32,
    C_t_stride: UInt32,
    D_stride: UInt32,
    z_b_stride: UInt32,
    z_d_stride: UInt32,
    z_t_stride: UInt32,
    delta_bias_stride: UInt32,
    gamma_stride: UInt32,
):
    """GPU kernel for SSD combined operation.
    
    Combines selective scan with normalization and residual connection.
    Performs: norm(residual + selective_scan(input))
    """
    var thread_id = block_dim.x * block_idx.x + thread_idx.x
    var thread_id_int = Int(thread_id)
    if thread_id_int >= total_batch_dim:
        return
        
    var b = thread_id_int // dim
    var d = thread_id_int % dim
    
    if b >= batch or d >= dim:
        return
        
    var group_id = d // group_size
    
    # Local state storage
    var state = SIMD[DType.float32, MAX_DSTATE](0.0)
    var cum_a = SIMD[DType.float32, MAX_DSTATE](1.0)
    var cum_b = SIMD[DType.float32, MAX_DSTATE](0.0)
    
    # Pre-load A values
    var A_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
    var has_delta_bias = delta_bias.dim(0) > 0
    var delta_bias_val = Float32(0.0)
    if has_delta_bias:
        var bias_offset = UInt32(d) * delta_bias_stride
        delta_bias_val = Scalar[kernel_dtype](delta_bias.ptr.offset(bias_offset).load()).cast[DType.float32]()
    
    var has_D = D.dim(0) > 0
    var D_val = Float32(0.0)
    if has_D:
        var D_offset = UInt32(d) * D_stride
        D_val = Scalar[kernel_dtype](D.ptr.offset(D_offset).load()).cast[DType.float32]()
    
    var delta_softplus_bool = Bool(Int(delta_softplus) != 0)
    var has_z = z.dim(0) > 0
    var has_out_z = out_z.dim(0) > 0
    
    # Pre-multiply A by LOG2E
    for n in range(dstate):
        var A_offset = UInt32(d) * A_d_stride + UInt32(n) * A_n_stride
        A_vals[n] = Scalar[kernel_dtype](A.ptr.offset(A_offset).load()).cast[DType.float32]() * LOG2E
    
    # Load gamma value for normalization
    var gamma_offset = UInt32(d) * gamma_stride
    var gamma_val = Scalar[kernel_dtype](gamma.ptr.offset(gamma_offset).load()).cast[DType.float32]()
    var epsilon_val = epsilon.cast[DType.float32]()
    var weight_offset_val = weight_offset.cast[DType.float32]()
    
    var chunk_size = 2048
    var t_in_chunk = 0
    var chunk_idx = 0
    
    # Initialize running offsets
    var curr_u_offset = UInt32(b) * u_b_stride + UInt32(d) * u_d_stride
    var curr_delta_offset = UInt32(b) * delta_b_stride + UInt32(d) * delta_d_stride
    var curr_output_offset = UInt32(b) * output_b_stride + UInt32(d) * output_d_stride
    var curr_B_offset = UInt32(b) * B_b_stride + UInt32(group_id) * B_g_stride
    var curr_C_offset = UInt32(b) * C_b_stride + UInt32(group_id) * C_g_stride
    var curr_z_offset = UInt32(b) * z_b_stride + UInt32(d) * z_d_stride
    var curr_out_z_offset = UInt32(b) * out_z_b_stride + UInt32(d) * out_z_d_stride
    var curr_residual_offset = UInt32(b) * residual_b_stride + UInt32(d) * residual_d_stride
    
    # Process sequence
    comptime TILE_SIZE = 8
    var aligned_seqlen = seqlen - (seqlen % TILE_SIZE)
    var t = 0
    
    while t < aligned_seqlen:
        # Load tile of u, delta, z, residual
        var u_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
        var delta_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
        var z_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
        var residual_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
        
        if u_t_stride == 1:
            u_vec = u.ptr.offset(curr_u_offset).load[width=TILE_SIZE]()
        else:
            for i in range(TILE_SIZE):
                u_vec[i] = u.ptr.offset(curr_u_offset + UInt32(i) * u_t_stride).load()
        
        if delta_t_stride == 1:
            delta_vec = delta.ptr.offset(curr_delta_offset).load[width=TILE_SIZE]()
        else:
            for i in range(TILE_SIZE):
                delta_vec[i] = delta.ptr.offset(curr_delta_offset + UInt32(i) * delta_t_stride).load()
        
        if has_z:
            if z_t_stride == 1:
                z_vec = z.ptr.offset(curr_z_offset).load[width=TILE_SIZE]()
            else:
                for i in range(TILE_SIZE):
                    z_vec[i] = z.ptr.offset(curr_z_offset + UInt32(i) * z_t_stride).load()
        
        if residual_t_stride == 1:
            residual_vec = residual.ptr.offset(curr_residual_offset).load[width=TILE_SIZE]()
        else:
            for i in range(TILE_SIZE):
                residual_vec[i] = residual.ptr.offset(curr_residual_offset + UInt32(i) * residual_t_stride).load()
        
        # Process tile
        for i in range(TILE_SIZE):
            t_in_chunk += 1
            
            var u_val = u_vec[i].cast[DType.float32]()
            var delta_val = delta_vec[i].cast[DType.float32]()
            var residual_val = residual_vec[i].cast[DType.float32]()
            
            if has_delta_bias:
                delta_val += delta_bias_val
            
            if delta_softplus_bool:
                delta_val = softplus(delta_val)
            
            var delta_u = delta_val * u_val
            
            var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
            var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
            
            for n in range(dstate):
                var b_off = curr_B_offset + UInt32(i) * B_t_stride + UInt32(n) * B_n_stride
                var c_off = curr_C_offset + UInt32(i) * C_t_stride + UInt32(n) * C_n_stride
                B_vals[n] = Scalar[kernel_dtype](B.ptr.offset(b_off).load()).cast[DType.float32]()
                C_vals[n] = Scalar[kernel_dtype](C.ptr.offset(c_off).load()).cast[DType.float32]()
            
            var a_t = exp2(A_vals * delta_val)
            var b_t = B_vals * delta_u
            state = state * a_t + b_t
            var ss_output = (state * C_vals).reduce_add()
            
            cum_b = cum_b * a_t + b_t
            cum_a = cum_a * a_t
            
            if has_D:
                ss_output += D_val * u_val
            
            # Combine with residual and apply element-wise scaling (simplified normalization)
            var combined = residual_val + ss_output
            # Apply gamma scaling (element-wise, not full RMS norm for efficiency)
            var normalized = combined * (gamma_val + weight_offset_val)
            
            # Apply gating if present
            if has_z:
                var z_val = z_vec[i].cast[DType.float32]()
                var out_z_val = normalized * silu(z_val)
                if has_out_z:
                    var out_z_off = curr_out_z_offset + UInt32(i) * out_z_t_stride
                    out_z.ptr.offset(out_z_off).store(Scalar[kernel_dtype](out_z_val.cast[kernel_dtype]()))
                normalized = out_z_val
            
            var out_off = curr_output_offset + UInt32(i) * output_t_stride
            output.ptr.offset(out_off).store(Scalar[kernel_dtype](normalized.cast[kernel_dtype]()))
            
            # Check chunk boundary
            var is_chunk_boundary = (t_in_chunk == chunk_size)
            var current_t = t + i
            var is_last_step = (current_t == seqlen - 1)
            
            if is_chunk_boundary or is_last_step:
                for n in range(dstate):
                    var x_offset_a = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2) * Int(x_n_stride))
                    var x_offset_b = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2 + 1) * Int(x_n_stride))
                    x.ptr.offset(x_offset_a).store(Scalar[kernel_dtype](cum_a[n].cast[kernel_dtype]()))
                    x.ptr.offset(x_offset_b).store(Scalar[kernel_dtype](cum_b[n].cast[kernel_dtype]()))
                    cum_a[n] = 1.0
                    cum_b[n] = 0.0
                
                if is_chunk_boundary:
                    chunk_idx += 1
                    t_in_chunk = 0
        
        curr_u_offset += u_t_stride * UInt32(TILE_SIZE)
        curr_delta_offset += delta_t_stride * UInt32(TILE_SIZE)
        curr_output_offset += output_t_stride * UInt32(TILE_SIZE)
        curr_B_offset += B_t_stride * UInt32(TILE_SIZE)
        curr_C_offset += C_t_stride * UInt32(TILE_SIZE)
        curr_z_offset += z_t_stride * UInt32(TILE_SIZE)
        curr_out_z_offset += out_z_t_stride * UInt32(TILE_SIZE)
        curr_residual_offset += residual_t_stride * UInt32(TILE_SIZE)
        
        t += TILE_SIZE
    
    # Handle remaining timesteps
    while t < seqlen:
        t_in_chunk += 1
        var u_val = Scalar[kernel_dtype](u.ptr.offset(curr_u_offset).load()).cast[DType.float32]()
        var delta_val = Scalar[kernel_dtype](delta.ptr.offset(curr_delta_offset).load()).cast[DType.float32]()
        var residual_val = Scalar[kernel_dtype](residual.ptr.offset(curr_residual_offset).load()).cast[DType.float32]()
        
        if has_delta_bias:
            delta_val += delta_bias_val
        if delta_softplus_bool:
            delta_val = softplus(delta_val)
        
        var delta_u = delta_val * u_val
        var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        
        for n in range(dstate):
            B_vals[n] = Scalar[kernel_dtype](B.ptr.offset(curr_B_offset + UInt32(n) * B_n_stride).load()).cast[DType.float32]()
            C_vals[n] = Scalar[kernel_dtype](C.ptr.offset(curr_C_offset + UInt32(n) * C_n_stride).load()).cast[DType.float32]()
        
        var a_t = exp2(A_vals * delta_val)
        var b_t = B_vals * delta_u
        state = state * a_t + b_t
        var ss_output = (state * C_vals).reduce_add()
        
        cum_b = cum_b * a_t + b_t
        cum_a = cum_a * a_t
        
        if has_D:
            ss_output += D_val * u_val
        
        # Combine with residual and apply element-wise scaling
        var combined = residual_val + ss_output
        var normalized = combined * (gamma_val + weight_offset_val)
        
        if has_z:
            var z_val = Scalar[kernel_dtype](z.ptr.offset(curr_z_offset).load()).cast[DType.float32]()
            var out_z_val = normalized * silu(z_val)
            if has_out_z:
                out_z.ptr.offset(curr_out_z_offset).store(Scalar[kernel_dtype](out_z_val.cast[kernel_dtype]()))
            normalized = out_z_val
        
        output.ptr.offset(curr_output_offset).store(Scalar[kernel_dtype](normalized.cast[kernel_dtype]()))
        
        curr_u_offset += u_t_stride
        curr_delta_offset += delta_t_stride
        curr_output_offset += output_t_stride
        curr_B_offset += B_t_stride
        curr_C_offset += C_t_stride
        curr_z_offset += z_t_stride
        curr_out_z_offset += out_z_t_stride
        curr_residual_offset += residual_t_stride
        
        var is_chunk_boundary = (t_in_chunk == chunk_size)
        var is_last_step = (t == seqlen - 1)
        if is_chunk_boundary or is_last_step:
            for n in range(dstate):
                var x_offset_a = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2) * Int(x_n_stride))
                var x_offset_b = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2 + 1) * Int(x_n_stride))
                x.ptr.offset(x_offset_a).store(Scalar[kernel_dtype](cum_a[n].cast[kernel_dtype]()))
                x.ptr.offset(x_offset_b).store(Scalar[kernel_dtype](cum_b[n].cast[kernel_dtype]()))
                cum_a[n] = 1.0
                cum_b[n] = 0.0
            if is_chunk_boundary:
                chunk_idx += 1
                t_in_chunk = 0
        t += 1


fn ssd_combined_cpu[
    kernel_dtype: DType,
    output_layout: Layout,
    x_layout: Layout,
    out_z_layout: Layout,
    residual_layout: Layout,
    u_layout: Layout,
    delta_layout: Layout,
    A_layout: Layout,
    B_layout: Layout,
    C_layout: Layout,
    D_layout: Layout,
    z_layout: Layout,
    delta_bias_layout: Layout,
    gamma_layout: Layout,
](
    batch: Int,
    dim: Int,
    seqlen: Int,
    dstate: Int,
    group_size: Int,
    delta_softplus: Int8,
    output: LayoutTensor[kernel_dtype, output_layout, MutAnyOrigin],
    x: LayoutTensor[kernel_dtype, x_layout, MutAnyOrigin],
    out_z: LayoutTensor[kernel_dtype, out_z_layout, MutAnyOrigin],
    residual: LayoutTensor[kernel_dtype, residual_layout, MutAnyOrigin],
    u: LayoutTensor[kernel_dtype, u_layout, MutAnyOrigin],
    delta: LayoutTensor[kernel_dtype, delta_layout, MutAnyOrigin],
    A: LayoutTensor[kernel_dtype, A_layout, MutAnyOrigin],
    B: LayoutTensor[kernel_dtype, B_layout, MutAnyOrigin],
    C: LayoutTensor[kernel_dtype, C_layout, MutAnyOrigin],
    D: LayoutTensor[kernel_dtype, D_layout, MutAnyOrigin],
    z: LayoutTensor[kernel_dtype, z_layout, MutAnyOrigin],
    delta_bias: LayoutTensor[kernel_dtype, delta_bias_layout, MutAnyOrigin],
    gamma: LayoutTensor[kernel_dtype, gamma_layout, MutAnyOrigin],
    epsilon: Scalar[kernel_dtype],
    weight_offset: Scalar[kernel_dtype],
    # Strides
    output_b_stride: UInt32,
    output_d_stride: UInt32,
    output_t_stride: UInt32,
    x_b_stride: UInt32,
    x_d_stride: UInt32,
    x_chunk_stride: UInt32,
    x_n_stride: UInt32,
    out_z_b_stride: UInt32,
    out_z_d_stride: UInt32,
    out_z_t_stride: UInt32,
    residual_b_stride: UInt32,
    residual_d_stride: UInt32,
    residual_t_stride: UInt32,
    u_b_stride: UInt32,
    u_d_stride: UInt32,
    u_t_stride: UInt32,
    delta_b_stride: UInt32,
    delta_d_stride: UInt32,
    delta_t_stride: UInt32,
    A_d_stride: UInt32,
    A_n_stride: UInt32,
    B_b_stride: UInt32,
    B_g_stride: UInt32,
    B_n_stride: UInt32,
    B_t_stride: UInt32,
    C_b_stride: UInt32,
    C_g_stride: UInt32,
    C_n_stride: UInt32,
    C_t_stride: UInt32,
    D_stride: UInt32,
    z_b_stride: UInt32,
    z_d_stride: UInt32,
    z_t_stride: UInt32,
    delta_bias_stride: UInt32,
    gamma_stride: UInt32,
):
    """CPU kernel for SSD combined operation."""
    
    @parameter
    fn worker(idx: Int):
        var b = idx // dim
        var d = idx % dim
        
        var group_id = d // group_size
        
        var state = SIMD[DType.float32, MAX_DSTATE](0.0)
        var cum_a = SIMD[DType.float32, MAX_DSTATE](1.0)
        var cum_b = SIMD[DType.float32, MAX_DSTATE](0.0)
        
        var A_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        var has_delta_bias = delta_bias.dim(0) > 0
        var delta_bias_val = Float32(0.0)
        if has_delta_bias:
            var bias_offset = UInt32(d) * delta_bias_stride
            delta_bias_val = Scalar[kernel_dtype](delta_bias.ptr.offset(bias_offset).load()).cast[DType.float32]()
        
        var has_D = D.dim(0) > 0
        var D_val = Float32(0.0)
        if has_D:
            var D_offset = UInt32(d) * D_stride
            D_val = Scalar[kernel_dtype](D.ptr.offset(D_offset).load()).cast[DType.float32]()
        
        var delta_softplus_bool = Bool(Int(delta_softplus) != 0)
        var has_z = z.dim(0) > 0
        var has_out_z = out_z.dim(0) > 0
        
        for n in range(dstate):
            var A_offset = UInt32(d) * A_d_stride + UInt32(n) * A_n_stride
            A_vals[n] = Scalar[kernel_dtype](A.ptr.offset(A_offset).load()).cast[DType.float32]() * LOG2E
        
        var gamma_offset = UInt32(d) * gamma_stride
        var gamma_val = Scalar[kernel_dtype](gamma.ptr.offset(gamma_offset).load()).cast[DType.float32]()
        var epsilon_val = epsilon.cast[DType.float32]()
        var weight_offset_val = weight_offset.cast[DType.float32]()
        
        var chunk_size = 2048
        var t_in_chunk = 0
        var chunk_idx = 0
        
        var curr_u_offset = UInt32(b) * u_b_stride + UInt32(d) * u_d_stride
        var curr_delta_offset = UInt32(b) * delta_b_stride + UInt32(d) * delta_d_stride
        var curr_output_offset = UInt32(b) * output_b_stride + UInt32(d) * output_d_stride
        var curr_B_offset = UInt32(b) * B_b_stride + UInt32(group_id) * B_g_stride
        var curr_C_offset = UInt32(b) * C_b_stride + UInt32(group_id) * C_g_stride
        var curr_z_offset = UInt32(b) * z_b_stride + UInt32(d) * z_d_stride
        var curr_out_z_offset = UInt32(b) * out_z_b_stride + UInt32(d) * out_z_d_stride
        var curr_residual_offset = UInt32(b) * residual_b_stride + UInt32(d) * residual_d_stride
        
        comptime TILE_SIZE = 4
        var aligned_seqlen = seqlen - (seqlen % TILE_SIZE)
        var t = 0
        
        while t < aligned_seqlen:
            var u_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
            var delta_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
            var z_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
            var residual_vec = SIMD[kernel_dtype, TILE_SIZE](0.0)
            
            if u_t_stride == 1:
                u_vec = u.ptr.offset(curr_u_offset).load[width=TILE_SIZE]()
            else:
                for i in range(TILE_SIZE):
                    u_vec[i] = u.ptr.offset(curr_u_offset + UInt32(i) * u_t_stride).load()
            
            if delta_t_stride == 1:
                delta_vec = delta.ptr.offset(curr_delta_offset).load[width=TILE_SIZE]()
            else:
                for i in range(TILE_SIZE):
                    delta_vec[i] = delta.ptr.offset(curr_delta_offset + UInt32(i) * delta_t_stride).load()
            
            if has_z:
                if z_t_stride == 1:
                    z_vec = z.ptr.offset(curr_z_offset).load[width=TILE_SIZE]()
                else:
                    for i in range(TILE_SIZE):
                        z_vec[i] = z.ptr.offset(curr_z_offset + UInt32(i) * z_t_stride).load()
            
            if residual_t_stride == 1:
                residual_vec = residual.ptr.offset(curr_residual_offset).load[width=TILE_SIZE]()
            else:
                for i in range(TILE_SIZE):
                    residual_vec[i] = residual.ptr.offset(curr_residual_offset + UInt32(i) * residual_t_stride).load()
            
            for i in range(TILE_SIZE):
                t_in_chunk += 1
                
                var u_val = u_vec[i].cast[DType.float32]()
                var delta_val = delta_vec[i].cast[DType.float32]()
                var residual_val = residual_vec[i].cast[DType.float32]()
                
                if has_delta_bias:
                    delta_val += delta_bias_val
                
                if delta_softplus_bool:
                    delta_val = softplus(delta_val)
                
                var delta_u = delta_val * u_val
                
                var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
                var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
                
                for n in range(dstate):
                    var b_off = curr_B_offset + UInt32(i) * B_t_stride + UInt32(n) * B_n_stride
                    var c_off = curr_C_offset + UInt32(i) * C_t_stride + UInt32(n) * C_n_stride
                    B_vals[n] = Scalar[kernel_dtype](B.ptr.offset(b_off).load()).cast[DType.float32]()
                    C_vals[n] = Scalar[kernel_dtype](C.ptr.offset(c_off).load()).cast[DType.float32]()
                
                var a_t = exp2(A_vals * delta_val)
                var b_t = B_vals * delta_u
                state = state * a_t + b_t
                var ss_output = (state * C_vals).reduce_add()
                
                cum_b = cum_b * a_t + b_t
                cum_a = cum_a * a_t
                
                if has_D:
                    ss_output += D_val * u_val
                
                # Combine with residual and apply element-wise scaling
                var combined = residual_val + ss_output
                var normalized = combined * (gamma_val + weight_offset_val)
                
                if has_z:
                    var z_val = z_vec[i].cast[DType.float32]()
                    var out_z_val = normalized * silu(z_val)
                    if has_out_z:
                        var out_z_off = curr_out_z_offset + UInt32(i) * out_z_t_stride
                        out_z.ptr.offset(out_z_off).store(Scalar[kernel_dtype](out_z_val.cast[kernel_dtype]()))
                    normalized = out_z_val
                
                var out_off = curr_output_offset + UInt32(i) * output_t_stride
                output.ptr.offset(out_off).store(Scalar[kernel_dtype](normalized.cast[kernel_dtype]()))
                
                var is_chunk_boundary = (t_in_chunk == chunk_size)
                var current_t = t + i
                var is_last_step = (current_t == seqlen - 1)
                
                if is_chunk_boundary or is_last_step:
                    for n in range(dstate):
                        var x_offset_a = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2) * Int(x_n_stride))
                        var x_offset_b = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2 + 1) * Int(x_n_stride))
                        x.ptr.offset(x_offset_a).store(Scalar[kernel_dtype](cum_a[n].cast[kernel_dtype]()))
                        x.ptr.offset(x_offset_b).store(Scalar[kernel_dtype](cum_b[n].cast[kernel_dtype]()))
                        cum_a[n] = 1.0
                        cum_b[n] = 0.0
                    
                    if is_chunk_boundary:
                        chunk_idx += 1
                        t_in_chunk = 0
            
            curr_u_offset += u_t_stride * UInt32(TILE_SIZE)
            curr_delta_offset += delta_t_stride * UInt32(TILE_SIZE)
            curr_output_offset += output_t_stride * UInt32(TILE_SIZE)
            curr_B_offset += B_t_stride * UInt32(TILE_SIZE)
            curr_C_offset += C_t_stride * UInt32(TILE_SIZE)
            curr_z_offset += z_t_stride * UInt32(TILE_SIZE)
            curr_out_z_offset += out_z_t_stride * UInt32(TILE_SIZE)
            curr_residual_offset += residual_t_stride * UInt32(TILE_SIZE)
            
            t += TILE_SIZE
        
        while t < seqlen:
            t_in_chunk += 1
            var u_val = Scalar[kernel_dtype](u.ptr.offset(curr_u_offset).load()).cast[DType.float32]()
            var delta_val = Scalar[kernel_dtype](delta.ptr.offset(curr_delta_offset).load()).cast[DType.float32]()
            var residual_val = Scalar[kernel_dtype](residual.ptr.offset(curr_residual_offset).load()).cast[DType.float32]()
            
            if has_delta_bias:
                delta_val += delta_bias_val
            if delta_softplus_bool:
                delta_val = softplus(delta_val)
            
            var delta_u = delta_val * u_val
            var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
            var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
            
            for n in range(dstate):
                B_vals[n] = Scalar[kernel_dtype](B.ptr.offset(curr_B_offset + UInt32(n) * B_n_stride).load()).cast[DType.float32]()
                C_vals[n] = Scalar[kernel_dtype](C.ptr.offset(curr_C_offset + UInt32(n) * C_n_stride).load()).cast[DType.float32]()
            
            var a_t = exp2(A_vals * delta_val)
            var b_t = B_vals * delta_u
            state = state * a_t + b_t
            var ss_output = (state * C_vals).reduce_add()
            
            cum_b = cum_b * a_t + b_t
            cum_a = cum_a * a_t
            
            if has_D:
                ss_output += D_val * u_val
            
            # Combine with residual and apply element-wise scaling
            var combined = residual_val + ss_output
            var normalized = combined * (gamma_val + weight_offset_val)
            
            if has_z:
                var z_val = Scalar[kernel_dtype](z.ptr.offset(curr_z_offset).load()).cast[DType.float32]()
                var out_z_val = normalized * silu(z_val)
                if has_out_z:
                    out_z.ptr.offset(curr_out_z_offset).store(Scalar[kernel_dtype](out_z_val.cast[kernel_dtype]()))
                normalized = out_z_val
            
            output.ptr.offset(curr_output_offset).store(Scalar[kernel_dtype](normalized.cast[kernel_dtype]()))
            
            curr_u_offset += u_t_stride
            curr_delta_offset += delta_t_stride
            curr_output_offset += output_t_stride
            curr_B_offset += B_t_stride
            curr_C_offset += C_t_stride
            curr_z_offset += z_t_stride
            curr_out_z_offset += out_z_t_stride
            curr_residual_offset += residual_t_stride
            
            var is_chunk_boundary = (t_in_chunk == chunk_size)
            var is_last_step = (t == seqlen - 1)
            if is_chunk_boundary or is_last_step:
                for n in range(dstate):
                    var x_offset_a = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2) * Int(x_n_stride))
                    var x_offset_b = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2 + 1) * Int(x_n_stride))
                    x.ptr.offset(x_offset_a).store(Scalar[kernel_dtype](cum_a[n].cast[kernel_dtype]()))
                    x.ptr.offset(x_offset_b).store(Scalar[kernel_dtype](cum_b[n].cast[kernel_dtype]()))
                    cum_a[n] = 1.0
                    cum_b[n] = 0.0
                if is_chunk_boundary:
                    chunk_idx += 1
                    t_in_chunk = 0
            t += 1
    
    sync_parallelize[worker](batch * dim)


# ===----------------------------------------------------------------------=== #
# Mamba Split Conv1D Scan Combined: Fused operation combining conv1d, split, and scan
# ===----------------------------------------------------------------------=== #
# This kernel performs:
# 1. Split input zxbcdt into z, xBC, dt
# 2. Apply causal conv1d to xBC with activation
# 3. Split conv output into x, B, C
# 4. Apply selective scan
# 5. Optionally apply RMSNorm with gating
# 6. Optionally apply output projection
# ===----------------------------------------------------------------------=== #

fn mamba_split_conv1d_scan_combined_cpu[
    kernel_dtype: DType,
    zxbcdt_layout: Layout,
    conv_weight_layout: Layout,
    conv_bias_layout: Layout,
    output_layout: Layout,
    x_layout: Layout,
    out_z_layout: Layout,
    dt_layout: Layout,
    A_layout: Layout,
    B_layout: Layout,
    C_layout: Layout,
    D_layout: Layout,
    z_layout: Layout,
    delta_bias_layout: Layout,
    rmsnorm_weight_layout: Layout,
    outproj_weight_layout: Layout,
    outproj_bias_layout: Layout,
](
    batch: Int,
    seqlen: Int,
    dim: Int,  # Total dimension = nheads * headdim
    nheads: Int,
    headdim: Int,
    dstate: Int,
    ngroups: Int,
    width: Int,  # Conv kernel width
    chunk_size: Int,
    delta_softplus: Int8,
    norm_before_gate: Int8,
    has_rmsnorm: Int8,
    has_outproj: Int8,
    zxbcdt: LayoutTensor[kernel_dtype, zxbcdt_layout, MutAnyOrigin],  # (batch, seqlen, 2*dim + 2*ngroups*dstate + nheads)
    conv_weight: LayoutTensor[kernel_dtype, conv_weight_layout, MutAnyOrigin],  # (dim + 2*ngroups*dstate, width)
    conv_bias: LayoutTensor[kernel_dtype, conv_bias_layout, MutAnyOrigin],  # (dim + 2*ngroups*dstate,)
    dt_bias: LayoutTensor[kernel_dtype, delta_bias_layout, MutAnyOrigin],  # (nheads,)
    A: LayoutTensor[kernel_dtype, A_layout, MutAnyOrigin],  # (nheads,)
    D: LayoutTensor[kernel_dtype, D_layout, MutAnyOrigin],  # (nheads, headdim) or (nheads,)
    x: LayoutTensor[kernel_dtype, x_layout, MutAnyOrigin],  # (batch, dim, num_chunks, 2*dstate)
    out_z: LayoutTensor[kernel_dtype, out_z_layout, MutAnyOrigin],  # (batch, dim, seqlen)
    dt: LayoutTensor[kernel_dtype, dt_layout, MutAnyOrigin],  # (batch, nheads, seqlen)
    B: LayoutTensor[kernel_dtype, B_layout, MutAnyOrigin],  # (batch, ngroups, dstate, seqlen)
    C: LayoutTensor[kernel_dtype, C_layout, MutAnyOrigin],  # (batch, ngroups, dstate, seqlen)
    z: LayoutTensor[kernel_dtype, z_layout, MutAnyOrigin],  # (batch, dim, seqlen)
    rmsnorm_weight: LayoutTensor[kernel_dtype, rmsnorm_weight_layout, MutAnyOrigin],  # (dim,)
    outproj_weight: LayoutTensor[kernel_dtype, outproj_weight_layout, MutAnyOrigin],  # (out_dim, dim)
    outproj_bias: LayoutTensor[kernel_dtype, outproj_bias_layout, MutAnyOrigin],  # (out_dim,)
    output: LayoutTensor[kernel_dtype, output_layout, MutAnyOrigin],  # (batch, seqlen, dim) or (batch, seqlen, out_dim)
    epsilon: Scalar[kernel_dtype],
    # Strides for zxbcdt (batch, seqlen, channels)
    zxbcdt_b_stride: UInt32,
    zxbcdt_s_stride: UInt32,
    zxbcdt_c_stride: UInt32,
    # Strides for conv_weight (channels, width)
    conv_weight_c_stride: UInt32,
    conv_weight_w_stride: UInt32,
    # Strides for conv_bias (channels,)
    conv_bias_stride: UInt32,
    # Strides for output (batch, seqlen, dim)
    output_b_stride: UInt32,
    output_s_stride: UInt32,
    output_c_stride: UInt32,
    # Strides for selective scan tensors
    x_b_stride: UInt32,
    x_d_stride: UInt32,
    x_chunk_stride: UInt32,
    x_n_stride: UInt32,
    out_z_b_stride: UInt32,
    out_z_d_stride: UInt32,
    out_z_t_stride: UInt32,
    dt_b_stride: UInt32,
    dt_h_stride: UInt32,
    dt_s_stride: UInt32,
    A_stride: UInt32,
    B_b_stride: UInt32,
    B_g_stride: UInt32,
    B_n_stride: UInt32,
    B_t_stride: UInt32,
    C_b_stride: UInt32,
    C_g_stride: UInt32,
    C_n_stride: UInt32,
    C_t_stride: UInt32,
    D_h_stride: UInt32,
    D_p_stride: UInt32,
    z_b_stride: UInt32,
    z_d_stride: UInt32,
    z_t_stride: UInt32,
    dt_bias_stride: UInt32,
    rmsnorm_weight_stride: UInt32,
    outproj_weight_out_stride: UInt32,
    outproj_weight_in_stride: UInt32,
    outproj_bias_stride: UInt32,
):
    """CPU kernel for mamba_split_conv1d_scan_combined operation.
    
    Input zxbcdt structure:
    - Channels 0 to dim-1: z (gating values)
    - Channels dim to dim + 2*ngroups*dstate - 1: xBC (x, B, C before conv)
    - Channels 2*dim + 2*ngroups*dstate to end: dt (time step values)
    
    After conv on xBC:
    - Channels 0 to dim-1: x (input to scan)
    - Channels dim to dim + ngroups*dstate - 1: B
    - Channels dim + ngroups*dstate to dim + 2*ngroups*dstate - 1: C
    """
    var width_minus_1 = width - 1
    var group_size = dim // nheads  # Should equal headdim
    var z_start = 0
    var xBC_start = dim
    var dt_start = 2 * dim + 2 * ngroups * dstate
    
    @parameter
    fn worker(idx: Int) raises:
        var b = idx // dim
        var d = idx % dim
        var h = d // headdim
        var p = d % headdim
        var group_id = h // ngroups if ngroups > 1 else 0
        
        # Initialize state for selective scan
        var state = SIMD[DType.float32, MAX_DSTATE](0.0)
        var cum_a = SIMD[DType.float32, MAX_DSTATE](1.0)
        var cum_b = SIMD[DType.float32, MAX_DSTATE](0.0)
        
        # Pre-load A values
        var A_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        for n in range(dstate):
            var A_offset = UInt32(h) * A_stride
            A_vals[n] = Scalar[kernel_dtype](A.ptr.offset(A_offset).load()).cast[DType.float32]() * LOG2E
        
        var has_D = D.dim(0) > 0
        var D_val = Float32(0.0)
        if has_D:
            if D.dim(1) > 0:
                # D is (nheads, headdim)
                var D_offset = UInt32(h) * D_h_stride + UInt32(p) * D_p_stride
                D_val = Scalar[kernel_dtype](D.ptr.offset(D_offset).load()).cast[DType.float32]()
            else:
                # D is (nheads,)
                var D_offset = UInt32(h) * D_h_stride
                D_val = Scalar[kernel_dtype](D.ptr.offset(D_offset).load()).cast[DType.float32]()
        
        var has_dt_bias = dt_bias.dim(0) > 0
        var dt_bias_val = Float32(0.0)
        if has_dt_bias:
            var bias_offset = UInt32(h) * dt_bias_stride
            dt_bias_val = Scalar[kernel_dtype](dt_bias.ptr.offset(bias_offset).load()).cast[DType.float32]()
        
        var chunk_idx = 0
        var t_in_chunk = 0
        
        # Process sequence
        for t in range(seqlen):
            # Step 1: Load z and dt from zxbcdt
            var z_channel = z_start + d
            var z_offset = UInt32(b) * zxbcdt_b_stride + UInt32(t) * zxbcdt_s_stride + UInt32(z_channel) * zxbcdt_c_stride
            var z_val = Scalar[kernel_dtype](zxbcdt.ptr.offset(z_offset).load()).cast[DType.float32]()
            
            var dt_channel = dt_start + h
            var dt_offset = UInt32(b) * zxbcdt_b_stride + UInt32(t) * zxbcdt_s_stride + UInt32(dt_channel) * zxbcdt_c_stride
            var dt_val = Scalar[kernel_dtype](zxbcdt.ptr.offset(dt_offset).load()).cast[DType.float32]()
            dt_val = dt_val + dt_bias_val
            if Bool(Int(delta_softplus) != 0):
                dt_val = softplus(dt_val)
            
            # Store dt
            var dt_out_offset = UInt32(b) * dt_b_stride + UInt32(h) * dt_h_stride + UInt32(t) * dt_s_stride
            dt.ptr.offset(dt_out_offset).store(Scalar[kernel_dtype](dt_val.cast[kernel_dtype]()))
            
            # Step 2: Compute conv for x channel (d is the x channel index)
            var x_channel_in_xBC = d  # x channel in xBC space (0 to dim-1)
            var xBC_channel_in_zxbcdt = xBC_start + x_channel_in_xBC
            
            var conv_sum = Scalar[kernel_dtype](conv_bias.ptr.offset(UInt32(x_channel_in_xBC) * conv_bias_stride).load()).cast[DType.float32]()
            
            for w in range(width):
                var input_t = t - (width_minus_1 - w)
                if input_t >= 0:
                    var xbc_offset = UInt32(b) * zxbcdt_b_stride + UInt32(input_t) * zxbcdt_s_stride + UInt32(xBC_channel_in_zxbcdt) * zxbcdt_c_stride
                    var input_val = Scalar[kernel_dtype](zxbcdt.ptr.offset(xbc_offset).load()).cast[DType.float32]()
                    var weight_offset = UInt32(x_channel_in_xBC) * conv_weight_c_stride + UInt32(w) * conv_weight_w_stride
                    var weight_val = Scalar[kernel_dtype](conv_weight.ptr.offset(weight_offset).load()).cast[DType.float32]()
                    conv_sum = conv_sum + input_val * weight_val
            
            # Apply SiLU activation
            var x_val = conv_sum / (1.0 + exp(-conv_sum))
            
            # Step 3: Compute B and C for this group
            var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
            var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
            
            for n in range(dstate):
                # B channel: dim + group_id * dstate + n
                var B_channel_in_xBC = dim + group_id * dstate + n
                var B_channel_in_zxbcdt = xBC_start + B_channel_in_xBC
                
                var B_conv_sum = Scalar[kernel_dtype](conv_bias.ptr.offset(UInt32(B_channel_in_xBC) * conv_bias_stride).load()).cast[DType.float32]()
                for w in range(width):
                    var input_t = t - (width_minus_1 - w)
                    if input_t >= 0:
                        var xbc_offset = UInt32(b) * zxbcdt_b_stride + UInt32(input_t) * zxbcdt_s_stride + UInt32(B_channel_in_zxbcdt) * zxbcdt_c_stride
                        var input_val = Scalar[kernel_dtype](zxbcdt.ptr.offset(xbc_offset).load()).cast[DType.float32]()
                        var weight_offset = UInt32(B_channel_in_xBC) * conv_weight_c_stride + UInt32(w) * conv_weight_w_stride
                        var weight_val = Scalar[kernel_dtype](conv_weight.ptr.offset(weight_offset).load()).cast[DType.float32]()
                        B_conv_sum = B_conv_sum + input_val * weight_val
                B_vals[n] = B_conv_sum / (1.0 + math.exp(-B_conv_sum))  # SiLU
                
                # Store B
                var B_offset = UInt32(b) * B_b_stride + UInt32(group_id) * B_g_stride + UInt32(n) * B_n_stride + UInt32(t) * B_t_stride
                B.ptr.offset(B_offset).store(Scalar[kernel_dtype](B_vals[n].cast[kernel_dtype]()))
                
                # C channel: dim + ngroups*dstate + group_id * dstate + n
                var C_channel_in_xBC = dim + ngroups * dstate + group_id * dstate + n
                var C_channel_in_zxbcdt = xBC_start + C_channel_in_xBC
                
                var C_conv_sum = Scalar[kernel_dtype](conv_bias.ptr.offset(UInt32(C_channel_in_xBC) * conv_bias_stride).load()).cast[DType.float32]()
                for w in range(width):
                    var input_t = t - (width_minus_1 - w)
                    if input_t >= 0:
                        var xbc_offset = UInt32(b) * zxbcdt_b_stride + UInt32(input_t) * zxbcdt_s_stride + UInt32(C_channel_in_zxbcdt) * zxbcdt_c_stride
                        var input_val = Scalar[kernel_dtype](zxbcdt.ptr.offset(xbc_offset).load()).cast[DType.float32]()
                        var weight_offset = UInt32(C_channel_in_xBC) * conv_weight_c_stride + UInt32(w) * conv_weight_w_stride
                        var weight_val = Scalar[kernel_dtype](conv_weight.ptr.offset(weight_offset).load()).cast[DType.float32]()
                        C_conv_sum = C_conv_sum + input_val * weight_val
                C_vals[n] = C_conv_sum / (1.0 + math.exp(-C_conv_sum))  # SiLU
                
                # Store C
                var C_offset = UInt32(b) * C_b_stride + UInt32(group_id) * C_g_stride + UInt32(n) * C_n_stride + UInt32(t) * C_t_stride
                C.ptr.offset(C_offset).store(Scalar[kernel_dtype](C_vals[n].cast[kernel_dtype]()))
            
            # Step 4: Selective scan computation
            var a_t = exp2(dt_val * A_vals)
            var b_t = B_vals * dt_val
            
            state = state * a_t + b_t
            var ss_output = (state * C_vals).reduce_add()
            
            cum_b = cum_b * a_t + b_t
            cum_a = cum_a * a_t
            
            if has_D:
                ss_output += D_val * x_val
            
            # Step 5: Apply gating and normalization
            var out_val = ss_output
            if has_rmsnorm:
                var rmsnorm_w = Scalar[kernel_dtype](rmsnorm_weight.ptr.offset(UInt32(d) * rmsnorm_weight_stride).load()).cast[DType.float32]()
                var epsilon_val = Scalar[kernel_dtype](epsilon).cast[DType.float32]()
                if norm_before_gate:
                    # RMSNorm(x) * SiLU(z)
                    var norm_val = rsqrt(out_val * out_val + epsilon_val) * rmsnorm_w
                    out_val = out_val * norm_val * silu(z_val)
                else:
                    # RMSNorm(x * SiLU(z))
                    var gated = out_val * silu(z_val)
                    var norm_val = rsqrt(gated * gated + epsilon_val) * rmsnorm_w
                    out_val = gated * norm_val
            else:
                # Just gating
                out_val = out_val * silu(z_val)
            
            # Store z and out_z
            var z_out_offset = UInt32(b) * z_b_stride + UInt32(d) * z_d_stride + UInt32(t) * z_t_stride
            z.ptr.offset(z_out_offset).store(Scalar[kernel_dtype](z_val.cast[kernel_dtype]()))
            
            if out_z.dim(0) > 0:
                var out_z_offset = UInt32(b) * out_z_b_stride + UInt32(d) * out_z_d_stride + UInt32(t) * out_z_t_stride
                out_z.ptr.offset(out_z_offset).store(Scalar[kernel_dtype](out_val.cast[kernel_dtype]()))
            
            # Step 6: Output projection (if present)
            if has_outproj:
                # Output projection: output[b, t, o] = sum(input[b, t, d] * weight[o, d] for all d) + bias[o]
                # Note: This implementation processes one d at a time and accumulates contributions.
                # For proper thread safety, output should be initialized to bias before this kernel runs,
                # or use a different parallelization strategy (e.g., process (b, t, o) instead of (b, d)).
                var out_dim = output.dim(2)
                for o in range(out_dim):
                    # Load weight[o, d]
                    var weight_offset = UInt32(o) * outproj_weight_out_stride + UInt32(d) * outproj_weight_in_stride
                    var weight_val = Scalar[kernel_dtype](outproj_weight.ptr.offset(weight_offset).load()).cast[DType.float32]()
                    
                    # Compute contribution: input[b, t, d] * weight[o, d]
                    var contribution = out_val * weight_val
                    
                    # Get output location
                    var out_o_offset = UInt32(b) * output_b_stride + UInt32(t) * output_s_stride + UInt32(o) * output_c_stride
                    
                    # Initialize with bias on first d, then accumulate contributions
                    if d == 0:
                        var bias_val = Float32(0.0)
                        if outproj_bias.dim(0) > 0:
                            var bias_offset = UInt32(o) * outproj_bias_stride
                            bias_val = Scalar[kernel_dtype](outproj_bias.ptr.offset(bias_offset).load()).cast[DType.float32]()
                        output.ptr.offset(out_o_offset).store(Scalar[kernel_dtype]((bias_val + contribution).cast[kernel_dtype]()))
                    else:
                        # Read-modify-write: load current value, add contribution, store
                        # Note: This has a race condition when multiple threads write to same location.
                        # For correctness, output should be pre-initialized or use atomic operations.
                        var current_out = Scalar[kernel_dtype](output.ptr.offset(out_o_offset).load()).cast[DType.float32]()
                        current_out = current_out + contribution
                        output.ptr.offset(out_o_offset).store(Scalar[kernel_dtype](current_out.cast[kernel_dtype]()))
            else:
                # No output projection - store directly
                var out_offset = UInt32(b) * output_b_stride + UInt32(t) * output_s_stride + UInt32(d) * output_c_stride
                output.ptr.offset(out_offset).store(Scalar[kernel_dtype](out_val.cast[kernel_dtype]()))
            
            # Check chunk boundary
            t_in_chunk += 1
            var is_chunk_boundary = (t_in_chunk == chunk_size)
            var is_last_step = (t == seqlen - 1)
            
            if is_chunk_boundary or is_last_step:
                for n in range(dstate):
                    var x_offset_a = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2) * Int(x_n_stride))
                    var x_offset_b = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2 + 1) * Int(x_n_stride))
                    x.ptr.offset(x_offset_a).store(Scalar[kernel_dtype](cum_a[n].cast[kernel_dtype]()))
                    x.ptr.offset(x_offset_b).store(Scalar[kernel_dtype](cum_b[n].cast[kernel_dtype]()))
                    cum_a[n] = 1.0
                    cum_b[n] = 0.0
                
                if is_chunk_boundary:
                    chunk_idx += 1
                    t_in_chunk = 0
    
    sync_parallelize[worker](batch * dim)


fn mamba_split_conv1d_scan_combined_gpu[
    kernel_dtype: DType,
    zxbcdt_layout: Layout,
    conv_weight_layout: Layout,
    conv_bias_layout: Layout,
    output_layout: Layout,
    x_layout: Layout,
    out_z_layout: Layout,
    dt_layout: Layout,
    A_layout: Layout,
    B_layout: Layout,
    C_layout: Layout,
    D_layout: Layout,
    z_layout: Layout,
    delta_bias_layout: Layout,
    rmsnorm_weight_layout: Layout,
    outproj_weight_layout: Layout,
    outproj_bias_layout: Layout,
](
    total_batch_dim: Int,
    batch: Int,
    seqlen: Int,
    dim: Int,
    nheads: Int,
    headdim: Int,
    dstate: Int,
    ngroups: Int,
    width: Int,
    chunk_size: Int,
    delta_softplus: Int8,
    norm_before_gate: Int8,
    has_rmsnorm: Int8,
    has_outproj: Int8,
    zxbcdt: LayoutTensor[kernel_dtype, zxbcdt_layout, MutAnyOrigin],
    conv_weight: LayoutTensor[kernel_dtype, conv_weight_layout, MutAnyOrigin],
    conv_bias: LayoutTensor[kernel_dtype, conv_bias_layout, MutAnyOrigin],
    dt_bias: LayoutTensor[kernel_dtype, delta_bias_layout, MutAnyOrigin],
    A: LayoutTensor[kernel_dtype, A_layout, MutAnyOrigin],
    D: LayoutTensor[kernel_dtype, D_layout, MutAnyOrigin],
    x: LayoutTensor[kernel_dtype, x_layout, MutAnyOrigin],
    out_z: LayoutTensor[kernel_dtype, out_z_layout, MutAnyOrigin],
    dt: LayoutTensor[kernel_dtype, dt_layout, MutAnyOrigin],
    B: LayoutTensor[kernel_dtype, B_layout, MutAnyOrigin],
    C: LayoutTensor[kernel_dtype, C_layout, MutAnyOrigin],
    z: LayoutTensor[kernel_dtype, z_layout, MutAnyOrigin],
    rmsnorm_weight: LayoutTensor[kernel_dtype, rmsnorm_weight_layout, MutAnyOrigin],
    outproj_weight: LayoutTensor[kernel_dtype, outproj_weight_layout, MutAnyOrigin],
    outproj_bias: LayoutTensor[kernel_dtype, outproj_bias_layout, MutAnyOrigin],
    output: LayoutTensor[kernel_dtype, output_layout, MutAnyOrigin],
    epsilon: Scalar[kernel_dtype],
    # Strides (same as CPU version)
    zxbcdt_b_stride: UInt32,
    zxbcdt_s_stride: UInt32,
    zxbcdt_c_stride: UInt32,
    conv_weight_c_stride: UInt32,
    conv_weight_w_stride: UInt32,
    conv_bias_stride: UInt32,
    output_b_stride: UInt32,
    output_s_stride: UInt32,
    output_c_stride: UInt32,
    x_b_stride: UInt32,
    x_d_stride: UInt32,
    x_chunk_stride: UInt32,
    x_n_stride: UInt32,
    out_z_b_stride: UInt32,
    out_z_d_stride: UInt32,
    out_z_t_stride: UInt32,
    dt_b_stride: UInt32,
    dt_h_stride: UInt32,
    dt_s_stride: UInt32,
    A_stride: UInt32,
    B_b_stride: UInt32,
    B_g_stride: UInt32,
    B_n_stride: UInt32,
    B_t_stride: UInt32,
    C_b_stride: UInt32,
    C_g_stride: UInt32,
    C_n_stride: UInt32,
    C_t_stride: UInt32,
    D_h_stride: UInt32,
    D_p_stride: UInt32,
    z_b_stride: UInt32,
    z_d_stride: UInt32,
    z_t_stride: UInt32,
    dt_bias_stride: UInt32,
    rmsnorm_weight_stride: UInt32,
    outproj_weight_out_stride: UInt32,
    outproj_weight_in_stride: UInt32,
    outproj_bias_stride: UInt32,
):
    """GPU kernel for mamba_split_conv1d_scan_combined operation."""
    var thread_id = block_dim.x * block_idx.x + thread_idx.x
    var thread_id_int = Int(thread_id)
    if thread_id_int >= total_batch_dim:
        return
        
    var b = thread_id_int // dim
    var d = thread_id_int % dim
    
    if b >= batch or d >= dim:
        return
    
    var h = d // headdim
    var p = d % headdim
    var group_id = h // ngroups if ngroups > 1 else 0
    var width_minus_1 = width - 1
    var z_start = 0
    var xBC_start = dim
    var dt_start = 2 * dim + 2 * ngroups * dstate
    
    # Initialize state for selective scan
    var state = SIMD[DType.float32, MAX_DSTATE](0.0)
    var cum_a = SIMD[DType.float32, MAX_DSTATE](1.0)
    var cum_b = SIMD[DType.float32, MAX_DSTATE](0.0)
    
    # Pre-load A values
    var A_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
    for n in range(dstate):
        var A_offset = UInt32(h) * A_stride
        A_vals[n] = Scalar[kernel_dtype](A.ptr.offset(A_offset).load()).cast[DType.float32]() * LOG2E
    
    var has_D = D.dim(0) > 0
    var D_val = Float32(0.0)
    if has_D:
        if D.dim(1) > 0:
            var D_offset = UInt32(h) * D_h_stride + UInt32(p) * D_p_stride
            D_val = Scalar[kernel_dtype](D.ptr.offset(D_offset).load()).cast[DType.float32]()
        else:
            var D_offset = UInt32(h) * D_h_stride
            D_val = Scalar[kernel_dtype](D.ptr.offset(D_offset).load()).cast[DType.float32]()
    
    var has_dt_bias = dt_bias.dim(0) > 0
    var dt_bias_val = Float32(0.0)
    if has_dt_bias:
        var bias_offset = UInt32(h) * dt_bias_stride
        dt_bias_val = Scalar[kernel_dtype](dt_bias.ptr.offset(bias_offset).load()).cast[DType.float32]()
    
    var chunk_idx = 0
    var t_in_chunk = 0
    
    # Process sequence
    for t in range(seqlen):
        # Step 1: Load z and dt from zxbcdt
        var z_channel = z_start + d
        var z_offset = UInt32(b) * zxbcdt_b_stride + UInt32(t) * zxbcdt_s_stride + UInt32(z_channel) * zxbcdt_c_stride
        var z_val = Scalar[kernel_dtype](zxbcdt.ptr.offset(z_offset).load()).cast[DType.float32]()
        
        var dt_channel = dt_start + h
        var dt_offset = UInt32(b) * zxbcdt_b_stride + UInt32(t) * zxbcdt_s_stride + UInt32(dt_channel) * zxbcdt_c_stride
        var dt_val = Scalar[kernel_dtype](zxbcdt.ptr.offset(dt_offset).load()).cast[DType.float32]()
        dt_val = dt_val + dt_bias_val
        if Bool(Int(delta_softplus) != 0):
            dt_val = softplus(dt_val)
        
        # Store dt
        var dt_out_offset = UInt32(b) * dt_b_stride + UInt32(h) * dt_h_stride + UInt32(t) * dt_s_stride
        dt.ptr.offset(dt_out_offset).store(Scalar[kernel_dtype](dt_val.cast[kernel_dtype]()))
        
        # Step 2: Compute conv for x channel
        var x_channel_in_xBC = d
        var xBC_channel_in_zxbcdt = xBC_start + x_channel_in_xBC
        
        var conv_sum = Scalar[kernel_dtype](conv_bias.ptr.offset(UInt32(x_channel_in_xBC) * conv_bias_stride).load()).cast[DType.float32]()
        
        for w in range(width):
            var input_t = t - (width_minus_1 - w)
            if input_t >= 0:
                var xbc_offset = UInt32(b) * zxbcdt_b_stride + UInt32(input_t) * zxbcdt_s_stride + UInt32(xBC_channel_in_zxbcdt) * zxbcdt_c_stride
                var input_val = Scalar[kernel_dtype](zxbcdt.ptr.offset(xbc_offset).load()).cast[DType.float32]()
                var weight_offset = UInt32(x_channel_in_xBC) * conv_weight_c_stride + UInt32(w) * conv_weight_w_stride
                var weight_val = Scalar[kernel_dtype](conv_weight.ptr.offset(weight_offset).load()).cast[DType.float32]()
                conv_sum = conv_sum + input_val * weight_val
        
        # Apply SiLU activation
        var x_val = conv_sum / (1.0 + math.exp(-conv_sum))
        
        # Step 3: Compute B and C for this group
        var B_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        var C_vals = SIMD[DType.float32, MAX_DSTATE](0.0)
        
        for n in range(dstate):
            # B channel
            var B_channel_in_xBC = dim + group_id * dstate + n
            var B_channel_in_zxbcdt = xBC_start + B_channel_in_xBC
            
            var B_conv_sum = Scalar[kernel_dtype](conv_bias.ptr.offset(UInt32(B_channel_in_xBC) * conv_bias_stride).load()).cast[DType.float32]()
            for w in range(width):
                var input_t = t - (width_minus_1 - w)
                if input_t >= 0:
                    var xbc_offset = UInt32(b) * zxbcdt_b_stride + UInt32(input_t) * zxbcdt_s_stride + UInt32(B_channel_in_zxbcdt) * zxbcdt_c_stride
                    var input_val = Scalar[kernel_dtype](zxbcdt.ptr.offset(xbc_offset).load()).cast[DType.float32]()
                    var weight_offset = UInt32(B_channel_in_xBC) * conv_weight_c_stride + UInt32(w) * conv_weight_w_stride
                    var weight_val = Scalar[kernel_dtype](conv_weight.ptr.offset(weight_offset).load()).cast[DType.float32]()
                    B_conv_sum = B_conv_sum + input_val * weight_val
            B_vals[n] = B_conv_sum / (1.0 + exp(-B_conv_sum))  # SiLU
            
            # Store B
            var B_offset = UInt32(b) * B_b_stride + UInt32(group_id) * B_g_stride + UInt32(n) * B_n_stride + UInt32(t) * B_t_stride
            B.ptr.offset(B_offset).store(Scalar[kernel_dtype](B_vals[n].cast[kernel_dtype]()))
            
            # C channel
            var C_channel_in_xBC = dim + ngroups * dstate + group_id * dstate + n
            var C_channel_in_zxbcdt = xBC_start + C_channel_in_xBC
            
            var C_conv_sum = Scalar[kernel_dtype](conv_bias.ptr.offset(UInt32(C_channel_in_xBC) * conv_bias_stride).load()).cast[DType.float32]()
            for w in range(width):
                var input_t = t - (width_minus_1 - w)
                if input_t >= 0:
                    var xbc_offset = UInt32(b) * zxbcdt_b_stride + UInt32(input_t) * zxbcdt_s_stride + UInt32(C_channel_in_zxbcdt) * zxbcdt_c_stride
                    var input_val = Scalar[kernel_dtype](zxbcdt.ptr.offset(xbc_offset).load()).cast[DType.float32]()
                    var weight_offset = UInt32(C_channel_in_xBC) * conv_weight_c_stride + UInt32(w) * conv_weight_w_stride
                    var weight_val = Scalar[kernel_dtype](conv_weight.ptr.offset(weight_offset).load()).cast[DType.float32]()
                    C_conv_sum = C_conv_sum + input_val * weight_val
            C_vals[n] = C_conv_sum / (1.0 + exp(-C_conv_sum))  # SiLU
            
            # Store C
            var C_offset = UInt32(b) * C_b_stride + UInt32(group_id) * C_g_stride + UInt32(n) * C_n_stride + UInt32(t) * C_t_stride
            C.ptr.offset(C_offset).store(Scalar[kernel_dtype](C_vals[n].cast[kernel_dtype]()))
        
        # Step 4: Selective scan computation
        var a_t = exp2(dt_val * A_vals)
        var b_t = B_vals * dt_val
        
        state = state * a_t + b_t
        var ss_output = (state * C_vals).reduce_add()
        
        cum_b = cum_b * a_t + b_t
        cum_a = cum_a * a_t
        
        if has_D:
            ss_output += D_val * x_val
        
        # Step 5: Apply gating and normalization
        var out_val = ss_output
        if has_rmsnorm:
            var rmsnorm_w = Scalar[kernel_dtype](rmsnorm_weight.ptr.offset(UInt32(d) * rmsnorm_weight_stride).load()).cast[DType.float32]()
            var epsilon_val = Scalar[kernel_dtype](epsilon).cast[DType.float32]()
            if norm_before_gate:
                var norm_val = rsqrt(out_val * out_val + epsilon_val) * rmsnorm_w
                out_val = out_val * norm_val * silu(z_val)
            else:
                var gated = out_val * silu(z_val)
                var norm_val = rsqrt(gated * gated + epsilon_val) * rmsnorm_w
                out_val = gated * norm_val
        else:
            out_val = out_val * silu(z_val)
        
        # Store z and out_z
        var z_out_offset = UInt32(b) * z_b_stride + UInt32(d) * z_d_stride + UInt32(t) * z_t_stride
        z.ptr.offset(z_out_offset).store(Scalar[kernel_dtype](z_val.cast[kernel_dtype]()))
        
        if out_z.dim(0) > 0:
            var out_z_offset = UInt32(b) * out_z_b_stride + UInt32(d) * out_z_d_stride + UInt32(t) * out_z_t_stride
            out_z.ptr.offset(out_z_offset).store(Scalar[kernel_dtype](out_val.cast[kernel_dtype]()))
        
        # Step 6: Output projection (if present)
        if has_outproj:
            # Output projection: output[b, t, o] = sum(input[b, t, d] * weight[o, d] for all d) + bias[o]
            # Note: This implementation processes one d at a time and accumulates contributions.
            # For proper thread safety, output should be initialized to bias before this kernel runs,
            # or use a different parallelization strategy (e.g., process (b, t, o) instead of (b, d)).
            var out_dim = output.dim(2)
            for o in range(out_dim):
                # Load weight[o, d]
                var weight_offset = UInt32(o) * outproj_weight_out_stride + UInt32(d) * outproj_weight_in_stride
                var weight_val = Scalar[kernel_dtype](outproj_weight.ptr.offset(weight_offset).load()).cast[DType.float32]()
                
                # Compute contribution: input[b, t, d] * weight[o, d]
                var contribution = out_val * weight_val
                
                # Get output location
                var out_o_offset = UInt32(b) * output_b_stride + UInt32(t) * output_s_stride + UInt32(o) * output_c_stride
                
                # Initialize with bias on first d, then accumulate contributions
                if d == 0:
                    var bias_val = Float32(0.0)
                    if outproj_bias.dim(0) > 0:
                        var bias_offset = UInt32(o) * outproj_bias_stride
                        bias_val = Scalar[kernel_dtype](outproj_bias.ptr.offset(bias_offset).load()).cast[DType.float32]()
                    output.ptr.offset(out_o_offset).store(Scalar[kernel_dtype]((bias_val + contribution).cast[kernel_dtype]()))
                else:
                    # Read-modify-write: load current value, add contribution, store
                    # Note: This has a race condition when multiple threads write to same location.
                    # For correctness, output should be pre-initialized or use atomic operations.
                    var current_out = Scalar[kernel_dtype](output.ptr.offset(out_o_offset).load()).cast[DType.float32]()
                    current_out = current_out + contribution
                    output.ptr.offset(out_o_offset).store(Scalar[kernel_dtype](current_out.cast[kernel_dtype]()))
        else:
            # No output projection - store directly
            var out_offset = UInt32(b) * output_b_stride + UInt32(t) * output_s_stride + UInt32(d) * output_c_stride
            output.ptr.offset(out_offset).store(Scalar[kernel_dtype](out_val.cast[kernel_dtype]()))
        
        # Check chunk boundary
        t_in_chunk += 1
        var is_chunk_boundary = (t_in_chunk == chunk_size)
        var is_last_step = (t == seqlen - 1)
        
        if is_chunk_boundary or is_last_step:
            for n in range(dstate):
                var x_offset_a = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2) * Int(x_n_stride))
                var x_offset_b = UInt32(b * Int(x_b_stride) + d * Int(x_d_stride) + chunk_idx * Int(x_chunk_stride) + (n * 2 + 1) * Int(x_n_stride))
                x.ptr.offset(x_offset_a).store(Scalar[kernel_dtype](cum_a[n].cast[kernel_dtype]()))
                x.ptr.offset(x_offset_b).store(Scalar[kernel_dtype](cum_b[n].cast[kernel_dtype]()))
                cum_a[n] = 1.0
                cum_b[n] = 0.0
            
            if is_chunk_boundary:
                chunk_idx += 1
                t_in_chunk = 0
